{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tika\n",
    "# Uncomment for Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstract(pdf):\n",
    "    raw = parser.from_file(pdf)\n",
    "    abstract_start = 0\n",
    "    abstract_end = 0\n",
    "    \n",
    "    for i in range(1000):\n",
    "        if(raw['content'][i:i+8]) == \"Abstract\":\n",
    "            abstract_start = i\n",
    "    \n",
    "    for j in range(3000):\n",
    "        if(raw['content'][j:j+14]) == \"1 Introduction\" or (raw['content'][j:j+15]) == \"1. Introduction\" or (raw['content'][j:j+14]) == \"1 INTRODUCTION\" or (raw['content'][j:j+14]) == \"1. INTRODUCTION\":\n",
    "            abstract_end = j\n",
    "    \n",
    "    paper_end = len(raw['content'])\n",
    "    return raw['content'][abstract_start : abstract_end]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper(pdf):\n",
    "    raw = parser.from_file(pdf)\n",
    "    abstract_start = 0\n",
    "    abstract_end = 0\n",
    "    \n",
    "    for i in range(1000):\n",
    "        if(raw['content'][i:i+8]) == \"Abstract\":\n",
    "            abstract_start = i\n",
    "    \n",
    "    for j in range(3000):\n",
    "        if(raw['content'][j:j+14]) == \"1 Introduction\" or (raw['content'][j:j+15]) == \"1. Introduction\" or (raw['content'][j:j+14]) == \"1 INTRODUCTION\" or (raw['content'][j:j+14]) == \"1. INTRODUCTION\":\n",
    "            abstract_end = j\n",
    "            \n",
    "    paper_end = len(raw['content'])\n",
    "    return raw['content'][abstract_end : paper_end]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "\n",
      "Transformer-based models are unable to pro-\n",
      "cess long sequences due to their self-attention\n",
      "operation, which scales quadratically with the\n",
      "sequence length. To address this limitation,\n",
      "we introduce the Longformer with an attention\n",
      "mechanism that scales linearly with sequence\n",
      "length, making it easy to process documents of\n",
      "thousands of tokens or longer. Longformer’s\n",
      "attention mechanism is a drop-in replacement\n",
      "for the standard self-attention and combines\n",
      "a local windowed attention with a task moti-\n",
      "vated global attention. Following prior work\n",
      "on long-sequence transformers, we evaluate\n",
      "Longformer on character-level language mod-\n",
      "eling and achieve state-of-the-art results on\n",
      "text8 and enwik8. In contrast to most\n",
      "prior work, we also pretrain Longformer and\n",
      "finetune it on a variety of downstream tasks.\n",
      "Our pretrained Longformer consistently out-\n",
      "performs RoBERTa on long document tasks\n",
      "and sets new state-of-the-art results on Wiki-\n",
      "Hop and TriviaQA.1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = '../Longformer.pdf'\n",
    "print(get_abstract(pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "\n",
      "Transformers (Vaswani et al., 2017) have achieved\n",
      "state-of-the-art results in a wide range of natu-\n",
      "ral language tasks including generative language\n",
      "modeling (Dai et al., 2019; Radford et al., 2019)\n",
      "and discriminative language understanding (De-\n",
      "vlin et al., 2019). This success is partly due to\n",
      "the self-attention component which enables the net-\n",
      "work to capture contextual information from the\n",
      "entire sequence. While powerful, the memory and\n",
      "computational requirements of self-attention grow\n",
      "quadratically with sequence length, making it infea-\n",
      "sible (or very expensive) to process long sequences\n",
      "on current hardware.\n",
      "\n",
      "To address this limitation, we present Long-\n",
      "former, a modified Transformer architecture with\n",
      "\n",
      "∗ Equal contribution.\n",
      "1https://github.com/allenai/longformer\n",
      "\n",
      "Figure 1: Longformer’s memory usage scales linearly\n",
      "with the sequence length, unlike the full self-attention\n",
      "mechanism that runs out of memory for long sequences\n",
      "on current GPUs. Longformer’s GPU-kernel is nearly\n",
      "as fast as the highly optimized full self-attention opera-\n",
      "tion, and nearly 6X faster than naive Pytorch.\n",
      "\n",
      "a self-attention operation that scales linearly with\n",
      "the sequence length, making it versatile for pro-\n",
      "cessing long documents (Fig. 1). This is an advan-\n",
      "tage for natural language tasks such as long docu-\n",
      "ment classification, question answering (QA), and\n",
      "coreference resolution, where existing approaches\n",
      "partition or shorten the long context into smaller\n",
      "sequences that fall within the typical 512 token\n",
      "limit of BERT-style pretrained models. Such parti-\n",
      "tioning could potentially result in loss of important\n",
      "cross-partition information, and to mitigate this\n",
      "problem, existing methods often rely on complex\n",
      "architectures to address such interactions. On the\n",
      "other hand, our proposed Longformer is able to\n",
      "build contextual representations of the entire con-\n",
      "text using multiple layers of attention, reducing the\n",
      "need for task-specific architectures.\n",
      "\n",
      "Recent work has addressed the computational in-\n",
      "efficiency of Transformers on long sequences (see\n",
      "Tab. 1). However, they primarily focus on autore-\n",
      "gressive language modeling, while the application\n",
      "of long document transformers to document-level\n",
      "NLP tasks in the transfer learning setting (Dai and\n",
      "Le, 2015; Peters et al., 2018; Howard and Ruder,\n",
      "2018; Devlin et al., 2019) has remained largely\n",
      "unexplored. We address this gap and show that\n",
      "Longformer’s attention mechanism can act as a\n",
      "\n",
      "ar\n",
      "X\n",
      "\n",
      "iv\n",
      ":2\n",
      "\n",
      "00\n",
      "4.\n",
      "\n",
      "05\n",
      "15\n",
      "\n",
      "0v\n",
      "1 \n",
      "\n",
      " [\n",
      "cs\n",
      "\n",
      ".C\n",
      "L\n",
      "\n",
      "] \n",
      " 1\n",
      "\n",
      "0 \n",
      "A\n",
      "\n",
      "pr\n",
      " 2\n",
      "\n",
      "02\n",
      "0\n",
      "\n",
      "https://github.com/allenai/longformer\n",
      "\n",
      "\n",
      "Model attention char-lm other pretrain\n",
      "matrix tasks\n",
      "\n",
      "Transformer-XL (2019) ltr yes no no\n",
      "Adaptive Span (2019) ltr yes no no\n",
      "Compressive (2020) ltr yes no no\n",
      "Reformer (2020) sparse yes no no\n",
      "Sparse (2019) sparse yes no no\n",
      "BP-Transformer (2019) sparse yes MT no\n",
      "Blockwise (2019) sparse no QA yes\n",
      "Our Longformer sparse yes multiple yes\n",
      "\n",
      "Table 1: Summary of prior work on adapting Trans-\n",
      "formers for long documents. ltr: left-to-right.\n",
      "\n",
      "drop-in replacement for the self-attention mecha-\n",
      "nism in pretrained Transformers, and leads to gains\n",
      "across a suite of document NLP tasks.\n",
      "\n",
      "Longformer’s attention mechanism is a combina-\n",
      "tion of a windowed local-context self-attention and\n",
      "an end task motivated global attention that encodes\n",
      "inductive bias about the task. Through ablations\n",
      "and controlled trials we show both attention types\n",
      "are essential – the local attention is primarily used\n",
      "to build contextual representations, while the global\n",
      "attention allows Longformer to build full sequence\n",
      "representations for prediction.\n",
      "\n",
      "We first evaluate Longformer on autoregressive\n",
      "character-level language modeling using a com-\n",
      "bination of windowed and a new dilated attention\n",
      "pattern, allowing the model to process sequences of\n",
      "up to 32K characters on modern GPUs. We achieve\n",
      "state-of-the-art results on text8 and enwik8\n",
      "benchmark datasets, demonstrating the effective-\n",
      "ness of Longformer in long document modeling.\n",
      "\n",
      "Then, to evaluate Longformer’s ability to re-\n",
      "place the full self-attention operation of existing\n",
      "pretrained models, we pretrain it with the masked\n",
      "language modeling (MLM) objective, continuing\n",
      "from the RoBERTa (Liu et al., 2019) released\n",
      "checkpoint. After pretraining, we apply it to\n",
      "downstream language tasks through finetuning and\n",
      "demonstrate that Longformer consistently outper-\n",
      "forms RoBERTa on a wide range of document-level\n",
      "natural language tasks including text classification,\n",
      "QA, and coreference resolution, achieving state-of-\n",
      "the-art results on two of these datasets.\n",
      "\n",
      "2 Related Work\n",
      "\n",
      "Long-Document Transformers Tab. 1 summa-\n",
      "rizes recent prior work on long documents. Two\n",
      "types of self-attention approaches have been ex-\n",
      "plored. The first is a left-to-right (ltr) approach that\n",
      "processes the document in chunks moving from\n",
      "\n",
      "left-to-right. While such models have been success-\n",
      "ful in autoregressive language modeling, they are\n",
      "unsuitable for transfer learning approaches with\n",
      "tasks that benefit from bidirectional context.\n",
      "\n",
      "Our work falls within the other general approach\n",
      "that defines some form of sparse attention pattern\n",
      "and avoids computing the full quadratic attention\n",
      "matrix multiplication. The model with the most\n",
      "similar attention pattern to ours is Sparse Trans-\n",
      "former (Child et al., 2019), which uses a form of\n",
      "dilated sliding window of blocks of size 8x8 pro-\n",
      "vided by BlockSparse (Gray et al., 2017). While\n",
      "both models use custom CUDA kernels, ours is im-\n",
      "plemented in TVM (Chen et al., 2018) (§3.2) mak-\n",
      "ing it more customizable and maintainable than\n",
      "BlockSparse which is implemented in C++, and\n",
      "designed for a specific versions of TensorFlow. We\n",
      "also introduce additional task motivated global at-\n",
      "tention patterns suitable for common NLP tasks\n",
      "(§3) and show they are essential for good perfor-\n",
      "mance in the transfer learning setting.\n",
      "\n",
      "A few models tried tasks other than autoregres-\n",
      "sive language modeling, which is a step forward\n",
      "because arguably focusing on language modeling\n",
      "as the primary evaluation has led to the develop-\n",
      "ment of models with limited applicability. BP-\n",
      "Transformer (Ye et al., 2019) evaluated on machine\n",
      "translation (MT), but didn’t explore the pretrain-\n",
      "finetune setting. Blockwise attention (Qiu et al.,\n",
      "2019) pretrained their models and evaluated on\n",
      "question answering (QA). However, the evaluation\n",
      "is limited as it doesn’t include language modeling,\n",
      "and the QA datasets are of relatively short docu-\n",
      "ments,2 therefore the effectiveness of this model\n",
      "on long document tasks remains unexplored.\n",
      "\n",
      "Task-specific Models for Long Documents\n",
      "Many task-specific approaches have been devel-\n",
      "oped to workaround the 512 limit of pretrained\n",
      "transformer models like BERT. The simplest ap-\n",
      "proach just truncates the document, commonly\n",
      "used for classification (Xie et al., 2019). An-\n",
      "other approach chunks the document into chunks\n",
      "of length 512 (could be overlapping), processes\n",
      "each chunk separately, then combines the activa-\n",
      "tions with a task specific model (Joshi et al., 2019).\n",
      "A third approach popular for multihop and open\n",
      "domain QA tasks uses a two-stage model where\n",
      "the first stage retrieves relevant documents that are\n",
      "passed onto the second stage for answer extrac-\n",
      "\n",
      "2 SQuAD contexts typically fit within the 512 limit, and\n",
      "MRQA is constructed by dropping long-document examples.\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "(a) Full n2 attention (b) Sliding window attention (c) Dilated sliding window (d) Global+sliding window\n",
      "\n",
      "Figure 2: Comparing the full self-attention pattern and the configuration of attention patterns in our Longformer.\n",
      "\n",
      "tion (Clark and Gardner, 2017; Chen et al., 2017).\n",
      "All of these approaches suffer from information\n",
      "loss due to truncation or cascading errors from\n",
      "the two stage approach. In contrast, Longformer\n",
      "can process long sequences without truncating or\n",
      "chunking, allowing us to adopt a much simpler ap-\n",
      "proach that concatenates the available context and\n",
      "processes it in a single pass.\n",
      "\n",
      "3 Model\n",
      "\n",
      "The original Transformer model has a self-attention\n",
      "component with O(n2) time and memory complex-\n",
      "ity where n is the input sequence length and thus, is\n",
      "not efficient to scale to long inputs. To address this\n",
      "challenge, we sparsify the full self-attention matrix\n",
      "according to an “attention pattern” specifying pairs\n",
      "of input locations attending to one another. Unlike\n",
      "the full self-attention, our proposed attention pat-\n",
      "tern scales linearly with the input sequence, making\n",
      "it efficient for longer sequences. In the following,\n",
      "we discuss the design and implementation of this\n",
      "attention pattern.\n",
      "\n",
      "3.1 Attention Pattern\n",
      "\n",
      "Fig. 2 summarizes the configurations of our pro-\n",
      "posed attention pattern.\n",
      "\n",
      "Sliding Window Given the importance of local\n",
      "context (Kovaleva et al., 2019), our attention pat-\n",
      "tern employs a fixed-size window attention sur-\n",
      "rounding each token. Using multiple stacked lay-\n",
      "ers of such windowed attention results in a large\n",
      "receptive field, where top layers have access to\n",
      "all input locations and have the capacity to build\n",
      "representations that incorporate information across\n",
      "the entire input. More formally, in this attention\n",
      "pattern, given a fixed window size w, each token\n",
      "attends to 1\n",
      "\n",
      "2\n",
      "w tokens on each side (Fig. 2b). The\n",
      "\n",
      "computation complexity of this pattern is O(n×w),\n",
      "which scales linearly with input sequence length n.\n",
      "To make this attention pattern efficient, w should\n",
      "be small compared with n. However, as mentioned\n",
      "\n",
      "above, a model with typical multiple stacked trans-\n",
      "former will have a large receptive field. This is\n",
      "analogues to CNNs where stacking layers of small\n",
      "kernels leads to high level features that are built\n",
      "from a large portion of the input (receptive field)\n",
      "(Wu et al., 2019). In our case, with a transformer of\n",
      "` layers, the receptive field size is `×w (assuming\n",
      "w is fixed for all layers). Depending on the appli-\n",
      "cation, it might be helpful to use different values of\n",
      "w for each layer to balance between efficiency and\n",
      "model representation capacity (§4.1).\n",
      "\n",
      "Dilated Sliding Window To further increase the\n",
      "receptive field without increasing computation, the\n",
      "sliding window can be “dilated”. This is analogues\n",
      "to dilated CNNs (van den Oord et al., 2016) where\n",
      "the window has gaps of size dilation d (Fig. 2c).\n",
      "Assuming a fixed d and w for all layers, the recep-\n",
      "tive field is ` × d × w, which can reach tens of\n",
      "thousands of tokens even for small values of d.\n",
      "\n",
      "In multi-headed attention, each attention head\n",
      "computes a different attention score. We found set-\n",
      "tings with different dilation configurations per head\n",
      "improves performance by allowing some heads\n",
      "without dilation to focus on local context, while\n",
      "others with dilation focus on longer context.\n",
      "\n",
      "Global Attention In state-of-the-art BERT-style\n",
      "models for natural language tasks, the optimal in-\n",
      "put representation differs from language modeling\n",
      "and varies by task. For masked language modeling\n",
      "(MLM), the model uses local context to predict the\n",
      "masked word, while for classification, the model ag-\n",
      "gregates the representation of the whole sequence\n",
      "into a special token ([CLS] in case of BERT). For\n",
      "QA, the question and document are concatenated,\n",
      "allowing the model to compare the question with\n",
      "the document through self-attention.\n",
      "\n",
      "In our case, the windowed and dilated attention\n",
      "are not flexible enough to learn task-specific repre-\n",
      "sentations. Accordingly, we add “global attention”\n",
      "on few pre-selected input locations. Importantly,\n",
      "we make this attention operation symmetric: that is,\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "a token with a global attention attends to all tokens\n",
      "across the sequence, and all tokens in the sequence\n",
      "attend to it. Fig. 2d shows an example of a slid-\n",
      "ing window attention with global attention at a few\n",
      "tokens at custom locations. For example for clas-\n",
      "sification, global attention is used for the [CLS]\n",
      "token while in QA global attention is provided on\n",
      "all question tokens. Note that, since the number of\n",
      "such tokens is small relative and independent of n\n",
      "(the total number of tokens in the input sequence)\n",
      "the complexity of the combined local and global\n",
      "attention is still O(n).\n",
      "\n",
      "While specifying global attention is task specific,\n",
      "it is much simpler than existing task specific ap-\n",
      "proaches that chunk/shorten the input into smaller\n",
      "sequences and often use complex architecture to\n",
      "combine information across these chunks. Fur-\n",
      "ther, it increases the representational power of the\n",
      "model as it allows building contextual representa-\n",
      "tions across the entire sequence.\n",
      "\n",
      "Linear Projections for Global Attention Re-\n",
      "call that given the linear projections Q, K, V , the\n",
      "Transformer model (Vaswani et al., 2017) computes\n",
      "attention scores as follows:\n",
      "\n",
      "Attention(Q,K, V ) = softmax\n",
      "(\n",
      "QKT\n",
      "√\n",
      "dk\n",
      "\n",
      ")\n",
      "V (1)\n",
      "\n",
      "We use two sets of projections, Qs, Ks, Vs to com-\n",
      "pute attention scores of sliding window attention,\n",
      "and Qg, Kg, Vg to compute attention scores for the\n",
      "global attention. The additional projections provide\n",
      "flexibility to model the different types of attention,\n",
      "which we show is critical for best performance on\n",
      "downstream tasks. Qg, Kg, Vg are all initialized\n",
      "with values that match Qs, Ks, Vs.\n",
      "\n",
      "3.2 CUDA Kernels for our Attention Pattern\n",
      "In regular transformers, attention scores are com-\n",
      "puted as in Eqn. 1. The expensive operation is the\n",
      "matrix multiplication QKT because both Q and K\n",
      "have n (sequence length) projections.\n",
      "\n",
      "Our dilated sliding window attention pattern is\n",
      "not straightforward to implement using modern\n",
      "deep learning libraries. Implementing it requires a\n",
      "form of banded matrix multiplication (matrix mul-\n",
      "tiplication where the output is all zero except cer-\n",
      "tain diagonals) that is not supported in existing\n",
      "deep learning libraries like PyTorch/Tensorflow.\n",
      "In addition, naive implementations with loops is\n",
      "unusably slow. To address this challenge, we pro-\n",
      "vide a highly efficient custom CUDA kernel that\n",
      "\n",
      "implements these attention operations and allows\n",
      "parallelizing the operation on GPU threads.\n",
      "\n",
      "Tensor Virtual Machine (TVM) We build our\n",
      "custom CUDA kernel using TVM (Chen et al.,\n",
      "2018), a deep learning compiler stack that compiles\n",
      "high level description of a function into optimized\n",
      "device-specific code. Using TVM, we describe our\n",
      "form of banded matrix multiplication in high-level\n",
      "python constructs, then TVM generates the corre-\n",
      "sponding CUDA code and compiles it for GPUs.\n",
      "\n",
      "Fig. 1 compares the runtime and memory of the\n",
      "full self-attention, our TVM implementation of the\n",
      "dilated sliding window, and a naive implementation\n",
      "of this attention. It is clear the full attention im-\n",
      "plementation is fast but it takes significant amount\n",
      "of memory because it needs to store all n2 values.\n",
      "The naive implementation with loops is not mem-\n",
      "ory consuming because it only stores the non-zero\n",
      "values, however it is significantly slow and imprac-\n",
      "tical to use. Finally, our implementation is both\n",
      "fast and memory efficient because it only computes\n",
      "and stores the non-zero values.3\n",
      "\n",
      "4 Autoregressive Language Modeling\n",
      "\n",
      "Autoregressive or left-to-right language modeling\n",
      "is loosely defined as estimating the probability dis-\n",
      "tribution of an existing token/character given its\n",
      "previous tokens/characters in an input sequence.\n",
      "This task is considered one of the fundamental tasks\n",
      "in natural language and recent prior work on mod-\n",
      "eling long sequences using transformers has relied\n",
      "on this task as their primary evaluation (Dai et al.,\n",
      "2019; Rae et al., 2020; Sukhbaatar et al., 2019).\n",
      "Similarly, we develop and evaluate our model on\n",
      "autoregressive language modeling.\n",
      "\n",
      "4.1 Attention Pattern\n",
      "\n",
      "For autoregressive language modeling we use\n",
      "our dilated sliding window attention. Follow-\n",
      "ing Sukhbaatar et al. (2019) we use differing win-\n",
      "dow sizes across the layers. In particular, we use\n",
      "small window sizes for the lower layers and in-\n",
      "crease window sizes as we move to higher layers.\n",
      "This allows the top layers to learn higher-level rep-\n",
      "resentation of the entire sequence while having the\n",
      "\n",
      "3It is worth noting that theoretically, a perfectly optimized\n",
      "sliding window attention operation should be faster than the\n",
      "n2 computation. However, achieving this level of performance\n",
      "requires special knowledge of low-level GPU programming,\n",
      "similar to implementing a highly optimized matrix multipli-\n",
      "cation. Our current implementation is sufficiently fast and\n",
      "practical to use.\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "Model #Param Dev Test\n",
      "\n",
      "Dataset text8\n",
      "T12 (Al-Rfou et al., 2018) 44M - 1.18\n",
      "Adaptive (Sukhbaatar et al., 2019) 38M 1.05 1.11\n",
      "BP-Transformer (Ye et al., 2019) 39M - 1.11\n",
      "Our Longformer 41M 1.04 1.10\n",
      "\n",
      "Dataset enwik8\n",
      "T12 (Al-Rfou et al., 2018) 44M - 1.11\n",
      "Transformer-XL (Dai et al., 2019) 41M - 1.06\n",
      "Reformer (Kitaev et al., 2020) - - 1.05\n",
      "Adaptive (Sukhbaatar et al., 2019) 39M 1.04 1.02\n",
      "BP-Transformer (Ye et al., 2019) 38M - 1.02\n",
      "Our Longformer 41M 1.02 1.00\n",
      "\n",
      "Table 2: Small model BPC on text8 & enwik8\n",
      "\n",
      "Model #Param Test BPC\n",
      "\n",
      "Transformer-XL (18 layers) 88M 1.03\n",
      "Sparse (Child et al., 2019) ≈100M 0.99\n",
      "Transformer-XL (24 layers) 277M 0.99\n",
      "Adaptive (Sukhbaatar et al., 2019) 209M 0.98\n",
      "Compressive (Rae et al., 2020) 277M 0.97\n",
      "Our Longformer 102M 0.99\n",
      "\n",
      "Table 3: Performance of large models on enwik8\n",
      "\n",
      "lower layers capture local information. In addition,\n",
      "it provides balance between efficiency (smaller win-\n",
      "dow sizes are less computationally expensive due\n",
      "to fewer nonzero values) and performance (larger\n",
      "window sizes have richer representation power and\n",
      "often result in performance improvements).\n",
      "\n",
      "We do not use dilated sliding windows for lower\n",
      "layers to maximize their capacity to learn and uti-\n",
      "lize the immediate local context. For the higher\n",
      "layers, we use a small amount of increasing dila-\n",
      "tion only on 2 heads. This gives the model the\n",
      "ability to directly attend to distant tokens without\n",
      "sacrificing local context.\n",
      "\n",
      "4.2 Experiment Setup\n",
      "Task and Datasets We focus on character-level\n",
      "autoregressive language modeling because the se-\n",
      "quences are naturally longer than those of word-\n",
      "level language modeling, making them suitable\n",
      "for evaluating our model. We used text8 and\n",
      "enwik8 (Mahoney, 2009) for evaluation, two stan-\n",
      "dard and widely used character-level language mod-\n",
      "eling datasets.\n",
      "\n",
      "Training Ideally, we would like to train our\n",
      "model on the largest window size and sequence\n",
      "length we can fit in a modern GPU memory. How-\n",
      "ever, we found that the model needs a large number\n",
      "of gradient updates to learn the local context first;\n",
      "before learning to utilize longer context. To accom-\n",
      "\n",
      "modate this, we adopt a staged training procedure\n",
      "where we increase the attention window size and\n",
      "sequence length across multiple training phrases.\n",
      "In particular, in the first phase we start with a short\n",
      "sequence length and window size, then on each sub-\n",
      "sequent phase, we double the window size and the\n",
      "sequence length, and halve the learning rate. This\n",
      "makes training fast, while keeping the slow part\n",
      "(longest sequences and window sizes) to the end.\n",
      "We train the model over 5 total phases with start-\n",
      "ing sequence length of 2,048 and ending sequence\n",
      "length of 23,040 on the last phase (see Appendix A\n",
      "for detailed configurations of each phase).\n",
      "\n",
      "Evaluation At evaluation time, we are able to\n",
      "run our model on sequence of length 32,256. Fol-\n",
      "lowing prior work (Dai et al., 2019; Sukhbaatar\n",
      "et al., 2019), during evaluation we split the dataset\n",
      "into overlapping sequences of size 32,256 with a\n",
      "step of size 512, and report the performance on the\n",
      "last 512 tokens on the sequence.\n",
      "\n",
      "Hyperparameters Our model only specifies\n",
      "how the self-attention component works, and it\n",
      "is agnostic to the other design choices for the trans-\n",
      "former model. We use relative position embeddings\n",
      "with sinusoidal weights as in Dai et al. (2019). We\n",
      "use two different model sizes, a small (12 layers,\n",
      "512 hidden size) model as in Dai et al. (2019),\n",
      "and a large (30 layers, 512 hidden size) model as\n",
      "in Child et al. (2019). We employed mixed preci-\n",
      "sion training (floating points 16 and 32) using apex4\n",
      "\n",
      "to reduce memory consumption and speed-up train-\n",
      "ing. However, we kept the attention computation\n",
      "in fp32 to avoid numerical instability issues.5 We\n",
      "used gradient checkpointing (Chen et al., 2016) to\n",
      "reduce memory usage, and ran our experiments on\n",
      "48GB RTX8000 GPUs. Refer to Appendix A for a\n",
      "more detailed list of hyperparameters.\n",
      "\n",
      "4.2.1 Results\n",
      "Tab. 2 and 3 summarize evaluation results on\n",
      "text8 and enwik8 datasets. We achieve a new\n",
      "state-of-the-art on both text8 and enwik8 using\n",
      "the small models with BPC of 1.10 and 1.00 on\n",
      "text8 and enwik8 respectively, demonstrating\n",
      "the effectiveness of our model.\n",
      "\n",
      "For large models, given how expensive these\n",
      "experiments are, and following recent work (Ki-\n",
      "taev et al., 2020; Rae et al., 2020), we are only\n",
      "\n",
      "4https://github.com/NVIDIA/apex\n",
      "5We found that using fp16 in attention operation results in\n",
      "\n",
      "floating point overflow and NaNs in later stages of training.\n",
      "\n",
      "5\n",
      "\n",
      "https://github.com/NVIDIA/apex\n",
      "\n",
      "\n",
      "Model Dev BPC\n",
      "\n",
      "Decreasing w (from 512 to 32) 1.24\n",
      "Fixed w (= 230) 1.23\n",
      "Increasing w (from 32 to 512) 1.21\n",
      "\n",
      "No Dilation 1.21\n",
      "Dilation on 2 heads 1.20\n",
      "\n",
      "Table 4: Top: changing window size across layers. Bot-\n",
      "tom: with/without dilation (@ 150K steps on phase1)\n",
      "\n",
      "evaluating on enwik8. Tab. 3 shows that Long-\n",
      "former outperforms the comparable Transformer-\n",
      "XL model, matches the performance of the compa-\n",
      "rable Sparse Transformer (Child et al., 2019), and\n",
      "matches or slightly underperforms recent models\n",
      "that have more than twice the number of parameters.\n",
      "It is worth noting that Adaptive Span (Sukhbaatar\n",
      "et al., 2019) and Compressive Transformer (Rae\n",
      "et al., 2020) are not good fit for the pretraining-\n",
      "finetuning paradigm as discussed in §2.\n",
      "\n",
      "4.2.2 Ablation Study\n",
      "To show the importance of the design choices of\n",
      "our attention patterns, we tried different variants\n",
      "and report their controlled experiment results. To\n",
      "make the ablation study more manageable, we train\n",
      "each configuration for 150K steps6 with phase 1\n",
      "configuration on a small model on text8, then\n",
      "report the BPC performance on the dev set.\n",
      "\n",
      "The top of Tab. 4 demonstrates the impact of\n",
      "different ways of configuring the window sizes\n",
      "per layer. We observe that increasing the window\n",
      "size from the bottom to the top layer leads to the\n",
      "best performance, arranging them in the reverse\n",
      "way leads to worse performance, and using a fixed\n",
      "window size (the average of window sizes of the\n",
      "other configuration) leads to a performance that\n",
      "it is in between. The bottom of Tab. 4 shows the\n",
      "impact of adding dilation. Adding some dilation to\n",
      "two heads leads to some improvement compared\n",
      "with no dilation at all.\n",
      "\n",
      "5 Pretraining and Finetuning\n",
      "\n",
      "Current state-of-the-art systems for many NLP\n",
      "tasks finetune a pretrained model with task super-\n",
      "vision (e.g. BERT). One of our main motivations\n",
      "is to develop such a model suitable for long docu-\n",
      "ment tasks. To do so, we pretrained Longformer\n",
      "\n",
      "6An obvious caveat is that there is a chance the end per-\n",
      "formance will not agree with the performance at step 150K.\n",
      "However, this is a reasonable approximation that saves the\n",
      "huge cost of running all these configurations to completion.\n",
      "\n",
      "Source Tokens Avg doc len\n",
      "\n",
      "Books (Zhu et al., 2015) 0.5B 95.9K\n",
      "English Wikipedia 2.1B 506\n",
      "Realnews (Zellers et al., 2019) 1.8B 1.7K\n",
      "Stories (Trinh and Le, 2018) 2.1B 7.8K\n",
      "\n",
      "Table 5: Pretraining data\n",
      "\n",
      "on a document corpus and finetune it for six tasks,\n",
      "including classification, QA and coreference resolu-\n",
      "tion. The resulting model can process sequences up\n",
      "to 4,096 tokens long (8 times longer than BERT)7.\n",
      "\n",
      "We pretrain Longformer with masked language\n",
      "modeling (MLM), where the goal is to recover\n",
      "randomly masked tokens in a sequence. Since\n",
      "MLM pretraining is expensive, we continue pre-\n",
      "training from the RoBERTa (Liu et al., 2019) re-\n",
      "leased checkpoint, while only making the minimal\n",
      "changes necessary to support Longformer’s atten-\n",
      "tion mechanism. Note that our attention pattern can\n",
      "be plugged into any pretrained transformer model\n",
      "without the need to change the model architecture.\n",
      "\n",
      "Attention Pattern We use the sliding window\n",
      "attention with window size of 512 on all layers.\n",
      "This matches RoBERTa’s sequence length, and\n",
      "therefore uses the same amount of computation\n",
      "as RoBERTa.8\n",
      "\n",
      "Position Embeddings RoBERTa uses learned\n",
      "absolute position embeddings with the maximum\n",
      "position being 512. To support longer documents,\n",
      "we add extra position embeddings to support up to\n",
      "position 4,096. To leverage RoBERTa’s pretrained\n",
      "weights, instead of randomly initializing the new\n",
      "position embeddings, we initialize them by copying\n",
      "the 512 position embeddings from RoBERTa mul-\n",
      "tiple times as analysis of BERT’s attention heads\n",
      "shows a strong learned bias to attending to local\n",
      "context, including the previous or next token (Clark\n",
      "et al., 2019). Using the copy initialization preserves\n",
      "this local structure everywhere except at the parti-\n",
      "tion boundaries. Despite its simplicity, we found\n",
      "this to be a very effective (see Tab. 6), allowing\n",
      "Longformer pretraining to rapidly converge with a\n",
      "small number of gradient updates.\n",
      "\n",
      "7Sequences up to 16K are possible on current GPUs.\n",
      "8We tried adding the additional dilation pattern on a few\n",
      "\n",
      "heads as in §4.1 but found it to hurt performance, likely\n",
      "because it is not compatible with the pretrained RoBERTa\n",
      "weights. We suspect that retraining such model from scratch\n",
      "might be needed to get improved performance\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "Pretraining Data In order to allow the model to\n",
      "learn long dependencies in pretraining, we com-\n",
      "piled a corpus of long documents. Some of these\n",
      "data sources were also included in the original\n",
      "RoBERTa pretraining including the Books corpus\n",
      "(Zhu et al., 2015) plus English Wikipedia. We\n",
      "additionally included one third of a subset of the\n",
      "Realnews dataset (Zellers et al., 2019) with doc-\n",
      "uments longer than 1,200 tokens as well as one\n",
      "third of the Stories (Trinh and Le, 2018) corpus.\n",
      "Our goal was to include a mix of long and short\n",
      "documents to both allow the model to learn longer\n",
      "dependencies while not to forget information from\n",
      "the original RoBERTa pretraining. The statistics of\n",
      "the pretraining data is shown in Tab. 5.\n",
      "\n",
      "Continued MLM Pretraining We train9 two\n",
      "sizes of Longformer, a base model and a large\n",
      "model. Both models are trained for 65K gradi-\n",
      "ent updates with sequences length 4,096, batch size\n",
      "64 (218 tokens), maximum learning rate of 3e-5,\n",
      "linear warmup of 500 steps, followed by a power 3\n",
      "polynomial decay. The rest of the hyperparameters\n",
      "are the same as RoBERTa.\n",
      "\n",
      "Tab. 6 shows the BPC on the development set of\n",
      "our training corpus. The first row shows a 1.846\n",
      "BPC using RoBERTa-base, which is comparable\n",
      "to the 1.880 BPC reported on the RoBERTa paper\n",
      "on their corpus. This indicates our training corpus\n",
      "is from a distribution close to that used to train\n",
      "RoBERTa. The following two rows show the per-\n",
      "formance of Longformer before pretraining with\n",
      "randomly initialized position embeddings and with\n",
      "copied position embeddings. The significant differ-\n",
      "ence indicates the importance of the copy initial-\n",
      "ization, and the relative small difference between\n",
      "the RoBERTa BPC and the initialized BPC indi-\n",
      "cates that our sliding window attention is working\n",
      "well with the RoBERTa weights. The following\n",
      "two rows show the impact of continuing pretrain-\n",
      "ing. Traininig for 2K steps improves BPC from\n",
      "1.957 to 1.753, which further decreases to 1.705 af-\n",
      "ter 65K steps, demonstrating the model is learning\n",
      "to better utilize the sliding window attention and\n",
      "longer context. Similar patterns are observed with\n",
      "RoBERTa-large and Longformer-large.\n",
      "\n",
      "5.1 Tasks\n",
      "We apply Longformer to multiple long document\n",
      "tasks, including QA, coreference resolution and\n",
      "classification. Tab. 7 shows the evaluation datasets\n",
      "\n",
      "9using fairseq (Ott et al., 2019)\n",
      "\n",
      "Model base large\n",
      "\n",
      "RoBERTa (seqlen: 512) 1.846 1.496\n",
      "Longformer (seqlen: 4,096) 10.299 8.738\n",
      "\n",
      "+ copy position embeddings 1.957 1.597\n",
      "+ 2K gradient updates 1.753 1.414\n",
      "+ 65K gradient updates 1.705 1.358\n",
      "\n",
      "Table 6: MLM BPC for RoBERTa and various pre-\n",
      "trained model configurations.\n",
      "\n",
      "Wordpieces WH TQA HQA ON IMDB HY\n",
      "\n",
      "avg. 1,535 6,589 1,316 506 300 705\n",
      "95th pctl. 3,627 17,126 1,889 1,147 705 1,975\n",
      "\n",
      "Table 7: Average and 95th percentile of context length\n",
      "of datasets in wordpieces. WH: WikiHop, TQA: Triv-\n",
      "iaQA, HQA: HotpotQA, ON: OntoNotes, HY: Hyper-\n",
      "partisan news\n",
      "\n",
      "have contexts significantly longer than 512 word-\n",
      "pieces. Our primary goal is to evaluate whether\n",
      "our attention mechanism can act as a replace-\n",
      "ment for the standard self-attention mechanism in\n",
      "BERT style models, and to perform controlled tri-\n",
      "als against a strong baseline. We are also interested\n",
      "in evaluating whether we can replace complicated\n",
      "task specific models necessitated by BERT’s lim-\n",
      "ited context with simpler models that just concate-\n",
      "nate all available context into a single sequence.\n",
      "\n",
      "Our baseline is a RoBERTa based model that\n",
      "breaks the context into the longest possible seg-\n",
      "ment, passes each individually through RoBERTa,\n",
      "and concatenates the activations for further process-\n",
      "ing. For QA tasks, we also concatenate the question\n",
      "to each segment so that RoBERTa can condition\n",
      "it’s contextual representations of the context on\n",
      "the question. The Longformer variant replaces the\n",
      "RoBERTa self-attention mechanism with our win-\n",
      "dowed attention used during pretraining, plus a task\n",
      "motivated global attention. The global attention\n",
      "uses additional linear projections (§3.1).\n",
      "\n",
      "Question answering We consider three datasets\n",
      "with long contexts: WikiHop (Welbl et al., 2018),\n",
      "Wikipedia setting of TriviaQA (Joshi et al., 2017),\n",
      "and HotpotQA, distractor dev setting (Yang et al.,\n",
      "2018).10\n",
      "\n",
      "For WikiHop and TriviaQA we follow the sim-\n",
      "ple QA model of BERT (Devlin et al., 2019), and\n",
      "concatenate question and documents into one long\n",
      "sequence, run it through Longformer, then have a\n",
      "\n",
      "10We use the full version of TriviaQA and HotpotQA, not\n",
      "the simplified versions in MRQA (Fisch et al., 2019).\n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "QA Coref. Classification\n",
      "\n",
      "Model WikiHop TriviaQA HotpotQA OntoNotes IMDB Hyperpartisan\n",
      "\n",
      "RoBERTa-base 72.4 74.3 63.5 78.4 95.3 87.4\n",
      "Longformer-base 75.0 75.2 64.4 78.6 95.7 94.8\n",
      "\n",
      "Table 8: Summary of finetuning results on QA, coreference resolution, and document classification. Results are on\n",
      "the development sets comparing our Longformer-base with RoBERTa-base. TriviaQA, Hyperpartisan metrics are\n",
      "F1, WikiHop and IMDB use accuracy, HotpotQa is joint F1, OntoNotes is average F1.\n",
      "\n",
      "dataset-specific prediction layer. WikiHop uses a\n",
      "classification layer for the candidate while Trivi-\n",
      "aQA uses the loss function of Clark and Gardner\n",
      "(2017) to predict answer span. We include global\n",
      "attention to question tokens and answer candidates\n",
      "for WikiHop and to question tokens for TriviaQA.\n",
      "\n",
      "For HotpotQA, we experiment with two different\n",
      "models, a single stage multitask model that jointly\n",
      "predicts evidence and answer spans, and a two-\n",
      "stage model that first extracts evidence paragraphs\n",
      "and passes them to a second stage for answer extrac-\n",
      "tion. The single stage model combines a span ex-\n",
      "traction loss with a question type (yes/no/span) clas-\n",
      "sification head over the first token and an evidence\n",
      "extraction loss predicted from special tokens at the\n",
      "end of sentences and paragraphs. We also include\n",
      "global attention to sentence and paragraph tokens.\n",
      "Note that both approaches are significantly sim-\n",
      "pler than recent SOTA models that include pipeline\n",
      "approaches of multiple stages and customized ar-\n",
      "chitectures (e.g., (Tu et al., 2019; Chen et al., 2019;\n",
      "Tu et al., 2020; Groeneveld et al., 2020)). See Ap-\n",
      "pendix B for further details about the models and\n",
      "hyperparameters.\n",
      "\n",
      "Coreference Resolution We use OntoNotes\n",
      "(Pradhan et al., 2012), and the model from Joshi\n",
      "et al. (2019), a modification of the system from\n",
      "Lee et al. (2018) to replace ELMo with BERT. The\n",
      "Longformer system is a straightforward adaption\n",
      "of the baseline model by replacing RoBERTa with\n",
      "Longformer and extending the sequence length.\n",
      "We didn’t use global attention for this task.\n",
      "\n",
      "Document Classification We evaluate on IMDB\n",
      "(Maas et al., 2011) and Hyperpartisan news detec-\n",
      "tion (Kiesel et al., 2019) datasets.11 IMDB is a\n",
      "standard sentiment classification datasets consist-\n",
      "ing of movie reviews. While most documents in\n",
      "this dataset are short, about 13.6% of them are\n",
      "\n",
      "11For Hyperpartisan we split the training data into\n",
      "train/dev/test sets using standard 90/10/10 splits and per-\n",
      "formed each experiment five times with different seeds to\n",
      "control variability associated with the small dataset.\n",
      "\n",
      "larger than 512 wordpieces (Tab. 7). Documents\n",
      "in Hyperpartisan are relatively long, and it is small\n",
      "with only 645 documents making it a good test for\n",
      "Longformer’s ability to adapt to limited data. We\n",
      "use global attention on the [CLS] token.\n",
      "\n",
      "5.2 Results\n",
      "\n",
      "Main Result Tab. 8 summarizes the results of all\n",
      "our finetuning experiments. We observe that Long-\n",
      "former consistently outperforms the RoBERTa\n",
      "baseline. Its performance gain is especially ob-\n",
      "vious for tasks that require long context such as\n",
      "WikiHop and Hyperpartisan. For TriviaQA, the\n",
      "improvement is more modest as the local context\n",
      "is often sufficient to answer the question. In the\n",
      "case of HotpotQA, the supporting fact auxiliary\n",
      "supervision allows models to easily find relevant\n",
      "contexts and then focus on local context, leading to\n",
      "smaller gains. This is contrasted with WikiHop that\n",
      "only includes distant supervision of intermediate\n",
      "reasoning chains, where our approach excels by\n",
      "reasoning over the entire context. On the IMDB\n",
      "and OntoNotes datasets the performance gains are\n",
      "smaller. For IMDB, the majority of the dataset\n",
      "consists of short documents and thus it is expected\n",
      "to see smaller improvements. For OntoNotes, we\n",
      "found that the distance between any two mentions\n",
      "is typically quite small so that a baseline that pro-\n",
      "cesses smaller chunks separately is able to stitch\n",
      "together mentions into coreference chains without\n",
      "considering cross chunk interactions.\n",
      "\n",
      "Longformer-large for QA We also evaluate the\n",
      "performance of Longformer-large on long context\n",
      "QA tasks. Tab. 9 shows that our Longformer-large\n",
      "achieves new state-of-the-art results on WikiHop\n",
      "and TriviaQA by large margins (3.6 and 4 points\n",
      "respectively). Tab. 10 summarizes results of Hot-\n",
      "potQA, and, as expected, using Longformer-large\n",
      "improves the result compared to Longformer-base.\n",
      "The two-stage model improves the results even\n",
      "further, likely because of the increased capacity\n",
      "that allows each stage to specialize on one task.\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "Model WikiHop TriviaQA\n",
      "\n",
      "Current SOTA 78.3 73.3\n",
      "Longformer-large 81.9 77.3\n",
      "\n",
      "Table 9: Leaderboard results of Longformer-large\n",
      "\n",
      "Model ans. supp. joint\n",
      "\n",
      "RoBERTa-base 73.5 83.4 63.5\n",
      "Longformer-base 74.3 84.4 64.4\n",
      "Longformer-large 78.8 86.0 69.5\n",
      "Longformer-large (2 stage) 81.0 85.8 71.4\n",
      "TAP2 (Glaß et al., 2019) (not GNN) 79.4 86.2 -\n",
      "HGN (Fang et al., 2019) 81.0 87.9 73.0\n",
      "C2F Reader (Shao et al., 2020) - - 73.9\n",
      "\n",
      "Table 10: HotpotQA results, distractor setting on the\n",
      "dev set. All numbers are F1 scores.13\n",
      "\n",
      "This model matches performance of TAP2 (Glaß\n",
      "et al., 2019), which is the the best performing sin-\n",
      "gle model that doesn’t use a form of graph neural\n",
      "networks (GNN; Kipf and Welling, 2017). How-\n",
      "ever, our model is simpler than TAP2 where they\n",
      "use a three-stage approach consisting of extracting\n",
      "paragraphs, evidence sentences and finally answer\n",
      "spans with an additional specialized span selection\n",
      "pretraining. All the better performing models for\n",
      "HotpotQA (Shao et al., 2020; Fang et al., 2019)\n",
      "use multi-stage approaches plus GNNs or graph\n",
      "network of entities, which seem to encode an im-\n",
      "portant inductive bias for the task.12 Furthermore,\n",
      "the 10 paragraphs come from 10 different docu-\n",
      "ments and the concatenated context is therefore not\n",
      "coherent. In this case we suspect that Longformer\n",
      "has less chance to use information from pretrain-\n",
      "ing due to difference between the pretraining and\n",
      "finetuning input structure. Additional pretraining\n",
      "tasks could help further improve results.\n",
      "\n",
      "5.3 Ablations on WikiHop\n",
      "Tab. 11 presents an ablation study for WikiHop. All\n",
      "results use Longformer-base, trained for 5 epochs\n",
      "with identical hyperparameters except where noted.\n",
      "Longformer benefits from longer sequences, global\n",
      "attention, separate projection matrices for global\n",
      "attention, MLM pretraining, and longer training.\n",
      "In addition, when configured as in RoBERTa-base\n",
      "(seqlen: 512, and n2 attention) Longformer per-\n",
      "forms slightly worse then RoBERTa-base, confirm-\n",
      "\n",
      "12We can encode this inductive bias using global attention\n",
      "over entities, but we leave this for future work.\n",
      "\n",
      "13 We report numbers from dev set scores of published\n",
      "papers. Missing values are not publicly available. Leaderboard\n",
      "test results will be included in the future.\n",
      "\n",
      "Model Accuracy / ∆\n",
      "\n",
      "Longformer (seqlen: 4,096) 73.8\n",
      "\n",
      "RoBERTa-base (seqlen: 512) 72.4 / -1.4\n",
      "Longformer (seqlen: 4,096, 15 epochs) 75.0 / +1.2\n",
      "Longformer (seqlen: 512, attention: n2) 71.7 / -2.1\n",
      "Longformer (seqlen: 512, attention: window) 68.8 / -5.0\n",
      "Longformer (seqlen: 2,048) 73.1 / -0.7\n",
      "Longformer (no MLM pretraining) 73.2 / -0.6\n",
      "Longformer (no linear proj.) 72.2 / -1.6\n",
      "Longformer (no linear proj. no global atten.) 65.5 / -8.3\n",
      "\n",
      "Table 11: WikiHop development set ablations\n",
      "\n",
      "ing that performance gains are not due to additional\n",
      "pretraining.\n",
      "\n",
      "6 Conclusion and Future Work\n",
      "\n",
      "We present Longformer, a transformer-based model\n",
      "that is scalable for processing long documents\n",
      "and that makes it easy to perform a wide range\n",
      "of document-level NLP tasks without chunk-\n",
      "ing/shortening the long input and without com-\n",
      "plex architecture to combine information across\n",
      "these chunks. Longformer employs an attention\n",
      "pattern that combines local and global information\n",
      "while also scaling linearly with the sequence length.\n",
      "Longformer achieves state-of-the-art results on the\n",
      "character-level language modeling tasks of text8\n",
      "and enwik8. When pretrained, Longformer con-\n",
      "sistently outperforms RoBERTa on long document\n",
      "tasks and sets new state-of-the-art results on Wiki-\n",
      "Hop and TriviaQA. For future work, we would like\n",
      "to explore other attention patterns that are more\n",
      "efficient by dynamically adapting to the input. We\n",
      "also would like to apply our model to other relevant\n",
      "long document tasks such as summarization.\n",
      "\n",
      "Acknowledgment\n",
      "\n",
      "We would like to thank Noah Smith, Dan Weld,\n",
      "Dirk Groeneveld, Kyle Lo, Daniel King and Doug\n",
      "Downey for helpful discussions and feedback, and\n",
      "the AI2 infrastructure team for technical support.\n",
      "\n",
      "References\n",
      "\n",
      "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy\n",
      "Guo, and Llion Jones. 2018. Character-level lan-\n",
      "guage modeling with deeper self-attention. In AAAI.\n",
      "\n",
      "Danqi Chen, Adam Fisch, Jason Weston, and Antoine\n",
      "Bordes. 2017. Reading wikipedia to answer open-\n",
      "domain questions. In ACL.\n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "Jifan Chen, Shih-Ting Lin, and Greg Durrett. 2019.\n",
      "Multi-hop question answering via reasoning chains.\n",
      "ArXiv, abs/1910.02610.\n",
      "\n",
      "Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin\n",
      "Zheng, Eddie Yan, Haichen Shen, Meghan Cowan,\n",
      "Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018.\n",
      "TVM: An automated end-to-end optimizing com-\n",
      "piler for deep learning. In 13th USENIX Symposium\n",
      "on Operating Systems Design and Implementation\n",
      "(OSDI 18), pages 578–594.\n",
      "\n",
      "Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\n",
      "Guestrin. 2016. Training deep nets with sublinear\n",
      "memory cost. ArXiv, abs/1604.06174.\n",
      "\n",
      "Rewon Child, Scott Gray, Alec Radford, and Ilya\n",
      "Sutskever. 2019. Generating long sequences with\n",
      "sparse transformers. ArXiv, abs/1904.10509.\n",
      "\n",
      "Christopher Clark and Matt Gardner. 2017. Simple\n",
      "and effective multi-paragraph reading comprehen-\n",
      "sion. In ACL.\n",
      "\n",
      "Kevin Clark, Urvashi Khandelwal, Omer Levy, and\n",
      "Christopher D. Manning. 2019. What does bert\n",
      "look at? an analysis of bert’s attention. ArXiv,\n",
      "abs/1906.04341.\n",
      "\n",
      "Andrew M Dai and Quoc V Le. 2015. Semi-supervised\n",
      "sequence learning. In Advances in Neural Informa-\n",
      "tion Processing Systems 28.\n",
      "\n",
      "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\n",
      "bonell, Quoc V. Le, and Ruslan Salakhutdinov. 2019.\n",
      "Transformer-xl: Attentive language models beyond\n",
      "a fixed-length context. In ACL.\n",
      "\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2019. Bert: Pre-training of deep\n",
      "bidirectional transformers for language understand-\n",
      "ing. In NAACL-HLT.\n",
      "\n",
      "Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang\n",
      "Wang, and Jing jing Liu. 2019. Hierarchical graph\n",
      "network for multi-hop question answering. ArXiv,\n",
      "abs/1911.03631.\n",
      "\n",
      "Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eu-\n",
      "nsol Choi, and Danqi Chen. 2019. MRQA 2019\n",
      "shared task: Evaluating generalization in reading\n",
      "comprehension. In Proceedings of 2nd Machine\n",
      "Reading for Reading Comprehension (MRQA) Work-\n",
      "shop at EMNLP.\n",
      "\n",
      "Michael Glaß, Alfio Massimiliano Gliozzo, Rishav\n",
      "Chakravarti, Anthony Ferritto, Lin Pan, Gaudani\n",
      "Bhargav, Dinesh Garg, and Avirup Sil. 2019. Span\n",
      "selection pre-training for question answering. ArXiv,\n",
      "abs/1909.04120.\n",
      "\n",
      "Scott Gray, Alec Radford, and Diederik P. Kingma.\n",
      "2017. Gpu kernels for block-sparse weights.\n",
      "\n",
      "Dirk Groeneveld, Tushar Khot, Mausam, and Ashish\n",
      "Sabhwaral. 2020. A simple yet strong pipeline for\n",
      "HotpotQA. arXiv preprint.\n",
      "\n",
      "Jeremy Howard and Sebastian Ruder. 2018. Universal\n",
      "language model fine-tuning for text classification. In\n",
      "ACL.\n",
      "\n",
      "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\n",
      "Zettlemoyer. 2017. Triviaqa: A large scale distantly\n",
      "supervised challenge dataset for reading comprehen-\n",
      "sion. In ACL.\n",
      "\n",
      "Mandar Joshi, Omer Levy, Luke Zettlemoyer, and\n",
      "Daniel Weld. 2019. BERT for coreference resolu-\n",
      "tion: Baselines and analysis. In EMNLP-IJCNLP.\n",
      "\n",
      "Johannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\n",
      "manuel Vincent, Payam Adineh, David Corney,\n",
      "Benno Stein, and Martin Potthast. 2019. SemEval-\n",
      "2019 task 4: Hyperpartisan news detection. In\n",
      "Proceedings of the 13th International Workshop on\n",
      "Semantic Evaluation, pages 829–839, Minneapo-\n",
      "lis, Minnesota, USA. Association for Computational\n",
      "Linguistics.\n",
      "\n",
      "Thomas N Kipf and Max Welling. 2017. Semi-\n",
      "supervised classification with graph convolutional\n",
      "networks. ICLR.\n",
      "\n",
      "Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n",
      "2020. Reformer: The efficient transformer. In\n",
      "ICLR.\n",
      "\n",
      "Olga V. Kovaleva, Alexey Romanov, Anna Rogers, and\n",
      "Anna Rumshisky. 2019. Revealing the dark secrets\n",
      "of bert. In EMNLP/IJCNLP.\n",
      "\n",
      "Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018.\n",
      "Higher-order coreference resolution with coarse-to-\n",
      "fine inference. In NAACL.\n",
      "\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\n",
      "dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n",
      "Luke Zettlemoyer, and Veselin Stoyanov. 2019.\n",
      "Roberta: A robustly optimized bert pretraining ap-\n",
      "proach. ArXiv, abs/1907.11692.\n",
      "\n",
      "Andrew L. Maas, Raymond E. Daly, Peter T. Pham,\n",
      "Dan Huang, Andrew Y. Ng, and Christopher Potts.\n",
      "2011. Learning word vectors for sentiment analy-\n",
      "sis. In Proceedings of the 49th Annual Meeting of\n",
      "the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies, pages 142–150, Port-\n",
      "land, Oregon, USA. Association for Computational\n",
      "Linguistics.\n",
      "\n",
      "Matt Mahoney. 2009. Large text compression bench-\n",
      "mark.\n",
      "\n",
      "Aäron van den Oord, Sander Dieleman, Heiga Zen,\n",
      "Karen Simonyan, Oriol Vinyals, Alex Graves,\n",
      "Nal Kalchbrenner, Andrew W. Senior, and Koray\n",
      "Kavukcuoglu. 2016. Wavenet: A generative model\n",
      "for raw audio. In SSW.\n",
      "\n",
      "Myle Ott, Sergey Edunov, Alexei Baevski, Angela\n",
      "Fan, Sam Gross, Nathan Ng, David Grangier, and\n",
      "Michael Auli. 2019. fairseq: A fast, extensible\n",
      "toolkit for sequence modeling. In Proceedings of\n",
      "NAACL-HLT 2019: Demonstrations.\n",
      "\n",
      "10\n",
      "\n",
      "https://doi.org/10.18653/v1/S19-2145\n",
      "https://doi.org/10.18653/v1/S19-2145\n",
      "http://www.aclweb.org/anthology/P11-1015\n",
      "http://www.aclweb.org/anthology/P11-1015\n",
      "\n",
      "\n",
      "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\n",
      "Gardner, Christopher Clark, Kenton Lee, and Luke\n",
      "Zettlemoyer. 2018. Deep contextualized word repre-\n",
      "sentations.\n",
      "\n",
      "Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,\n",
      "Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-\n",
      "2012 shared task: Modeling multilingual unre-\n",
      "stricted coreference in OntoNotes. In Joint Confer-\n",
      "ence on EMNLP and CoNLL - Shared Task, pages\n",
      "1–40, Jeju Island, Korea. Association for Computa-\n",
      "tional Linguistics.\n",
      "\n",
      "Jiezhong Qiu, Hao Ma, Omer Levy, Scott Yih,\n",
      "Sinong Wang, and Jie Tang. 2019. Blockwise self-\n",
      "attention for long document understanding. ArXiv,\n",
      "abs/1911.02972.\n",
      "\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\n",
      "Dario Amodei, and Ilya Sutskever. 2019. Language\n",
      "models are unsupervised multitask learners.\n",
      "\n",
      "Jack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\n",
      "mar, and Timothy P. Lillicrap. 2020. Compressive\n",
      "transformers for long-range sequence modelling. In\n",
      "ICLR.\n",
      "\n",
      "Nan Shao, Yiming Cui, Ting Liu, Shijin Wang, and\n",
      "Guoping Hu. 2020. Is graph structure necessary for\n",
      "multi-hop reasoning? ArXiv, abs/2004.03096.\n",
      "\n",
      "Sainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\n",
      "janowski, and Armand Joulin. 2019. Adaptive at-\n",
      "tention span in transformers. In ACL.\n",
      "\n",
      "Trieu H. Trinh and Quoc V. Le. 2018. A sim-\n",
      "ple method for commonsense reasoning. ArXiv,\n",
      "abs/1806.02847.\n",
      "\n",
      "Ming Tu, Jinke Huang, Xiaodong He, and Bowen Zhou.\n",
      "2020. Graph sequential network for reasoning over\n",
      "sequences.\n",
      "\n",
      "Ming Tu, Kevin Huang, Guangtao Wang, Jing Huang,\n",
      "Xiaodong He, and Bufang Zhou. 2019. Select, an-\n",
      "swer and explain: Interpretable multi-hop reading\n",
      "comprehension over multiple documents. ArXiv,\n",
      "abs/1911.00484.\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. In NIPS.\n",
      "\n",
      "Johannes Welbl, Pontus Stenetorp, and Sebastian\n",
      "Riedel. 2018. Constructing datasets for multi-hop\n",
      "reading comprehension across documents. Transac-\n",
      "tions of the Association for Computational Linguis-\n",
      "tics, 6:287–302.\n",
      "\n",
      "Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\n",
      "and Michael Auli. 2019. Pay less attention with\n",
      "lightweight and dynamic convolutions. ArXiv,\n",
      "abs/1901.10430.\n",
      "\n",
      "Qizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang\n",
      "Luong, and Quoc V. Le. 2019. Unsupervised data\n",
      "augmentation for consistency training.\n",
      "\n",
      "Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shu\n",
      "xin Zheng, Chen Xing, Huishuai Zhang, Yanyan\n",
      "Lan, Li-Wei Wang, and Tie-Yan Liu. 2020. On layer\n",
      "normalization in the transformer architecture. ArXiv,\n",
      "abs/2002.04745.\n",
      "\n",
      "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\n",
      "gio, William W. Cohen, Ruslan Salakhutdinov, and\n",
      "Christopher D. Manning. 2018. Hotpotqa: A dataset\n",
      "for diverse, explainable multi-hop question answer-\n",
      "ing. In EMNLP.\n",
      "\n",
      "Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and\n",
      "Zheng Zhang. 2019. Bp-transformer: Modelling\n",
      "long-range context via binary partitioning. ArXiv,\n",
      "abs/1911.04070.\n",
      "\n",
      "Rowan Zellers, Ari Holtzman, Hannah Rashkin,\n",
      "Yonatan Bisk, Ali Farhadi, Franziska Roesner, and\n",
      "Yejin Choi. 2019. Defending against neural fake\n",
      "news. In NeurIPS.\n",
      "\n",
      "Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\n",
      "Salakhutdinov, Raquel Urtasun, Antonio Torralba,\n",
      "and Sanja Fidler. 2015. Aligning books and movies:\n",
      "Towards story-like visual explanations by watching\n",
      "movies and reading books. 2015 IEEE International\n",
      "Conference on Computer Vision (ICCV), pages 19–\n",
      "27.\n",
      "\n",
      "11\n",
      "\n",
      "https://www.aclweb.org/anthology/W12-4501\n",
      "https://www.aclweb.org/anthology/W12-4501\n",
      "https://www.aclweb.org/anthology/W12-4501\n",
      "\n",
      "\n",
      "A Character LM Hyperparameters\n",
      "\n",
      "We evaluate on text8 and enwik8, both contain\n",
      "100M characters from Wikipedia split into 90M,\n",
      "5M, 5M for train, dev, test. Our implementation\n",
      "is based on the Transformer-XL (Dai et al., 2019)\n",
      "code14 with the memory mechanism disabled. Our\n",
      "hyperparameters and stage configurations are listed\n",
      "in Tab. 12. Our CUDA kernel supports the autore-\n",
      "gressive mode where each token attends to a win-\n",
      "dow of previous tokens only. Our implementation\n",
      "also includes a version of the relative position em-\n",
      "bedding that is compatible with our dilated sliding\n",
      "window attention.\n",
      "\n",
      "We ran the small model experiments on 4\n",
      "RTX8000 GPUs for 16 days. For the large model,\n",
      "we ran experiments on 8 RTX8000 GPUs for 13\n",
      "days. Most of our hyperparameter search is similar\n",
      "to the ablation in Tab. 4 where we run the configu-\n",
      "ration for 150K steps on text8. We experimented\n",
      "with absolute position embeddings and learned po-\n",
      "sition embeddings, dropout values of [0.1, 0.2]\n",
      "(small model) and [0.1, 0.4] (large model), pre-\n",
      "layernorm and post-layernorm (Xiong et al., 2020),\n",
      "learning rate (LR) of phase1 of values [2.5e-5, 5e-\n",
      "4, 1e-4] constant and cosine LR schedules, and\n",
      "different configurations for dilation (on all heads,\n",
      "on 2 heads, no dilation). Number of gradient up-\n",
      "dates/phase reported in Tab. 12 is determined by\n",
      "running each phase until the validation BPC stops\n",
      "getting better.\n",
      "\n",
      "B Task specific model details\n",
      "\n",
      "All the QA and classification models are imple-\n",
      "mented using PyTorch-Lightning15.\n",
      "\n",
      "WikiHop Instances in WikiHop consist of: a\n",
      "question, answer candidates (ranging from two\n",
      "candidates to 79 candidates), supporting contexts\n",
      "(ranging from three paragraphs to 63 paragraphs),\n",
      "and the correct answer. The dataset does not pro-\n",
      "vide any intermediate annotation for the multihop\n",
      "reasoning chains, requiring models to instead infer\n",
      "them from the indirect answer supervision.\n",
      "\n",
      "To prepare the data for input to Longformer\n",
      "and RoBERTa, we first tokenize the question,\n",
      "answer candidates, and support contexts using\n",
      "RoBERTa’s wordpiece tokenizer. Then we\n",
      "\n",
      "14https://github.com/kimiyoung/\n",
      "transformer-xl\n",
      "\n",
      "15https://github.com/PyTorchLightning/\n",
      "pytorch-lightning\n",
      "\n",
      "concatenate the question and answer candi-\n",
      "dates with special tokens as [q] question\n",
      "[/q] [ent] candidate1 [/ent] ...\n",
      "[ent] candidateN [/ent]. The contexts\n",
      "are also concatenated using RoBERTa’s doc-\n",
      "ument delimiter tokens as separators: </s>\n",
      "context1 </s> ... </s> contextM\n",
      "</s>. The special tokens [q], [/q],\n",
      "[ent], [/ent] were added to the RoBERTa\n",
      "vocabulary and randomly initialized before task\n",
      "finetuning.\n",
      "\n",
      "After preparing the input data, we compute acti-\n",
      "vations from the top layer of each model as follows.\n",
      "We take the question and answer candidates and\n",
      "concatenate them to as much context as possible up\n",
      "to the model sequence length (512 for RoBERTa,\n",
      "4,096 for Longformer), run the sequence through\n",
      "the model, collect the output activations, and repeat\n",
      "until all of the context is exhausted (for all models\n",
      "except Longformer-large, where we just include\n",
      "the first 4,096 length sequence due to memory re-\n",
      "quirements). Then all activations for all chunks are\n",
      "concatenated into one long sequence. In the case of\n",
      "Longformer, we use global attention to the entire\n",
      "question and answer candidate sequence.\n",
      "\n",
      "For prediction, we attach a linear layer to each\n",
      "[ent] that outputs a single logit, average over all\n",
      "logits for each candidate across the chunks, apply\n",
      "a softmax and use the cross entropy loss with the\n",
      "correct answer candidate.\n",
      "\n",
      "Training used the Adam optimizer with linear\n",
      "warmup over 200 gradient updates to a maximum\n",
      "LR, and linear decay over the remainder of training.\n",
      "We used gradient accumulation to effective batch\n",
      "size of 32 instances, checking the development ac-\n",
      "curacy every 250 gradient updates and reported the\n",
      "maximum development accuracy. Other hyperpa-\n",
      "rameters (dropout, weight decay) were identical to\n",
      "RoBERTa pretraining.\n",
      "\n",
      "In general, we ran minimal hyperparameter trials,\n",
      "but for fair comparison between Longformer and\n",
      "RoBERTa ran an identical hyperparameter search\n",
      "with Longformer-base and RoBERTa-base. This\n",
      "consisted of a grid search of LR in [2e-5, 3e-5,\n",
      "5e-5] and number epochs in [5, 10, 15]. The\n",
      "best Longformer-base configuration used lr=3e-5,\n",
      "15 epochs. We ran two hyperparameter trials for\n",
      "Longformer-large, lr=3e-5 and number epochs in\n",
      "[5, 15] (the 5 epoch model had higher dev accuracy\n",
      "of 77.6, and was the single model submitted to the\n",
      "public leaderboard for test set evaluation). All mod-\n",
      "\n",
      "12\n",
      "\n",
      "https://github.com/kimiyoung/transformer-xl\n",
      "https://github.com/kimiyoung/transformer-xl\n",
      "https://github.com/PyTorchLightning/pytorch-lightning\n",
      "https://github.com/PyTorchLightning/pytorch-lightning\n",
      "\n",
      "\n",
      "Param Value\n",
      "\n",
      "Position Embeddings Relative and Sinusoidal as in Dai et al. (2019)\n",
      "Small model config 12 layers, 8 heads, 512 hidden size as in Dai et al. (2019)\n",
      "Large model config 30 layers, 8 heads, 512 hidden size as in Child et al. (2019)\n",
      "Optimizer AdamW\n",
      "Dropout 0.2 (small model), 0.4 (large model)\n",
      "Gradient clipping 0.25\n",
      "Weight Decay 0.01\n",
      "Layernorm Location pre-layernorm (Xiong et al., 2020)\n",
      "Activation GeLU\n",
      "Number of phases 5\n",
      "Phase 1 window sizes 32 (bottom layer) - 8,192 (top layer)\n",
      "Phase 5 window sizes 512 (bottom layer) - (top layer)\n",
      "Phase 1 sequence length 2,048\n",
      "Phase 5 sequence length 23,040 (gpu memory limit)\n",
      "Phase 1 LR 0.00025\n",
      "Phase 5 LR 000015625\n",
      "Batch size per phase 32, 32, 16, 16, 16\n",
      "#Steps per phase (small) 430K, 50k, 50k, 35k, 5k\n",
      "#Steps per phase (large) 350K, 25k, 10k, 5k, 5k\n",
      "Warmup 10% of the phase steps with maximum 10K steps\n",
      "LR scheduler constant throughout each phase\n",
      "Dilation (small model) 0 (layers 0-5), 1 (layers 6-7), 2 (layers 8-9), 3 (layers 10-11)\n",
      "Dilation (large model) 0 (layers 0-14), 1 (layers 15-19), 2 (layers 20-24), 3 (layers 25-29)\n",
      "Dilation heads 2 heads only\n",
      "\n",
      "Table 12: Hyperparameters for the best performing model for character-level language modeling\n",
      "\n",
      "els were trained on a single RTX8000 GPU, with\n",
      "Longformer-base taking about a day for 5 epochs.\n",
      "\n",
      "TriviaQA TriviaQA has more than 100K ques-\n",
      "tion, answer, document triplets for training. Doc-\n",
      "uments are Wikipedia articles, and answers are\n",
      "named entities mentioned in the article. The span\n",
      "that answers the question is not annotated, but it is\n",
      "found using simple text matching.\n",
      "\n",
      "Similar to WikiHop, we tokenize the question\n",
      "and the document using RoBERTa’s tokenizer,\n",
      "then form the input as [s] question [/s]\n",
      "document [/s]. We truncate the document at\n",
      "4,096 wordpiece to avoid it being very slow. After-\n",
      "wards, we get the activations from RoBERTa and\n",
      "Longformer similar to WikiHop (discussed above).\n",
      "We use global attention on all question tokens.\n",
      "\n",
      "For prediction, we add one layer that predicts the\n",
      "beginning and end of the answer span. Because of\n",
      "the distant supervision nature of the training data\n",
      "(no gold answer spans), we use the loss function\n",
      "of Clark and Gardner (2017) which works like an\n",
      "OR that the model only needs to get one answer\n",
      "span right, not all of them.\n",
      "\n",
      "Hyperparameters of the best configuration are\n",
      "listed in Tab. 13. All other hyperparameters are\n",
      "similar to RoBERTa’s. For hyperparameter search,\n",
      "we only tuned LR for the RoBERTa baseline and\n",
      "tried rates [3e-5, 5e-5, 1e-4], then used the best,\n",
      "which is 3e-5, for all subsequent experiments with\n",
      "\n",
      "no further tuning. We trained the Longformer-large\n",
      "with the best configuration once and submitted its\n",
      "output to the leaderboard. We ran our experiments\n",
      "on 32GB V100 GPUs. Small model takes 1 day to\n",
      "train on 4 GPUs, while large model takes 1 day on\n",
      "8 GPUs.\n",
      "\n",
      "HotpotQA HotpotQA dataset involves answer-\n",
      "ing questions from a set of 10 paragraphs from\n",
      "10 different Wikipedia articles where 2 paragraphs\n",
      "are relevant to the question and the rest are dis-\n",
      "tractors. It includes 2 tasks of answer span ex-\n",
      "traction and evidence sentence identification. Our\n",
      "model for HotpotQA combines both answer span\n",
      "extraction and evidence extraction in one joint\n",
      "model. We also experimented with a two-stage\n",
      "model with similar setup that uses longformer first\n",
      "for evidence extraction and then extract answers\n",
      "using a BERT-based QA model. The second stage\n",
      "of our two-stage model is based on Groeneveld\n",
      "et al. (2020). Similar to Wikihop and TriviaQA,\n",
      "to prepare the data for input to Longformer and\n",
      "RoBERTa, we concatenate question and then all\n",
      "the 10 paragraphs in one long context. We partic-\n",
      "ularly use the following input format with special\n",
      "tokens: “[CLS] [q] question [/q] [p]\n",
      "sent1,1 [s] sent1,2 [s] ... [p] sent2,1\n",
      "[s] sent2,2 [s] ...” where [s] and [p]\n",
      "are special tokens representing sentences and para-\n",
      "graphs. The special tokens were added to the\n",
      "\n",
      "13\n",
      "\n",
      "\n",
      "\n",
      "RoBERTa vocabulary and randomly initialized be-\n",
      "fore task finetuning. For Longformer, we use global\n",
      "attention to input tokens as well as sentence and\n",
      "paragraph tokens. For answer span extraction we\n",
      "use BERT’s QA model (Devlin et al., 2019) with\n",
      "addition of a question type (yes/no/span) classifi-\n",
      "cation head over the first special token ([CLS]).\n",
      "For evidence extraction we apply 2 layer feedfor-\n",
      "ward networks on top of the representations corre-\n",
      "sponding to sentence and paragraph tokens to get\n",
      "the corresponding evidence prediction scores and\n",
      "use binary cross entropy loss to train the model.\n",
      "We combine span, question classification, sentence,\n",
      "and paragraphs losses and train the model in a mul-\n",
      "titask way using linear combination of losses. Our\n",
      "experiments are done on RTX8000 GPUs and train-\n",
      "ing each epoch takes approximately half a day on\n",
      "4 GPUs. We trained the model using Adam opti-\n",
      "mizer with linear warmup (1000 steps) and linear\n",
      "decay. We used minimal hyperpareter tuning using\n",
      "LRs of 3e-5 and 5e-5 and epochs of 3 to 7 and\n",
      "found the model with LR of 3e-5 and 6 epochs to\n",
      "work best. We conduct the same hyperparameter\n",
      "search for the RoBERTa baseline as well. The rest\n",
      "of hyperparameters are reported in Tab 13.\n",
      "\n",
      "Param WikiHop TriviaQA HotpotQA\n",
      "\n",
      "Epochs 15 5 6\n",
      "LR 3e-5 3e-5 5e-5\n",
      "Warmup steps 200 1000 1000\n",
      "Batch size 32 32 32\n",
      "Optimizer Adam Adam Adam\n",
      "\n",
      "Table 13: Hyperparameters of the QA models. All mod-\n",
      "els use a similar scheduler with linear warmup and de-\n",
      "cay.\n",
      "\n",
      "Coreference model details The coreference\n",
      "model is a straightforward adaptation of the coarse-\n",
      "to-fine BERT based model from Joshi et al.\n",
      "(2019). After preprocessing each document with\n",
      "the RoBERTa wordpiece tokenizer, it splits each\n",
      "document into non-overlapping segments up to the\n",
      "maximum sequence length, then concatenates the\n",
      "activations for the coarse-to-fine clustering stage\n",
      "that forms coreference clusters. The maximum se-\n",
      "quence length was 384 for RoBERTa-base, chosen\n",
      "after three trials from [256, 384, 512] using the\n",
      "default hyperparameters in the original implemen-\n",
      "tation.16 For Longformer-base the sequence length\n",
      "was 4,096. Similar to the original implementation,\n",
      "\n",
      "16https://github.com/mandarjoshi90/coref\n",
      "\n",
      "different learning rates were used for the pretrained\n",
      "RoBERTa parameters and the randomly initialized\n",
      "task parameters. Using a larger learning rate in the\n",
      "task parameters allows the optimizer to adjust them\n",
      "farther from their randomly initialized values with-\n",
      "out destroying the information in the pretrained\n",
      "RoBERTa parameters.\n",
      "\n",
      "Hyperparameter searches were minimal and con-\n",
      "sisted of grid searches of RoBERTa LR in [1e-5,\n",
      "2e-5, 3e-5] and task LR in [1e-4, 2e-4, 3e-4] for\n",
      "both RoBERTa and Longformer for a fair compari-\n",
      "son. The best configuration for Longformer-base\n",
      "was RoBERTa lr=1e-5, task lr=1e-4. All other hy-\n",
      "perparameters were the same as in the original im-\n",
      "plementation. Training takes about 10 hours on a\n",
      "single GPU.\n",
      "\n",
      "Our implementation is a superhack that involves\n",
      "PyTorch and Tensorflow sharing a single process\n",
      "and GPU. To avoid re-implementing the com-\n",
      "plicated coarse-to-fine logic from Tensorflow in\n",
      "PyTorch (that involves a highly optimized cus-\n",
      "tom GPU kernel originally released by Lee et al.\n",
      "(2018)), we devised a system where the lower trans-\n",
      "former portion of the model passes activations and\n",
      "gradients back and forth between PyTorch and Ten-\n",
      "sorflow. The input tensors are first run through\n",
      "the transformer in PyTorch, the activations are col-\n",
      "lected from the top layer, transferred from GPU\n",
      "to CPU then from CPU to Tensorflow and back to\n",
      "GPU to run the coarse-to-fine clustering and com-\n",
      "pute the loss. Then gradients are back propogated\n",
      "in Tensorflow to the top of the transformer and\n",
      "the process reversed to transfer them to PyTorch\n",
      "for back propogation through the remainder of the\n",
      "model. Separate optimizers are maintained with\n",
      "identical LR schedules for parameter updates. The\n",
      "overhead in this approach is minimal compared to\n",
      "the overall cost of running the model.\n",
      "\n",
      "Text classification For classification, following\n",
      "BERT, we used a simple binary cross entropy loss\n",
      "on top of a first [CLS] token with addition of\n",
      "global attention to [CLS]. We used Adam opti-\n",
      "mizer with batch sizes of 32 and linear warmup\n",
      "and decay with warmup steps equal to 0.1 of the\n",
      "total training steps. For both IMDB and Hyperpar-\n",
      "tisan news we did grid search of LRs [3e-5, 5e-5]\n",
      "and epochs [10, 15, 20] and found the model with\n",
      "[3e-5] and epochs 15 to work best. Experiments\n",
      "were done on a single RTX8000 GPU.\n",
      "\n",
      "14\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = '../Longformer.pdf'\n",
    "print(get_paper(pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "\n",
      "Using the recent proposal for the observables in open string field theory, we\n",
      "explicitly compute the coupling of closed string tachyon and massless states\n",
      "with the open string states up to level two. Using these couplings, we then\n",
      "calculate the tree level S-matrix elements of two closed string tachyons or two\n",
      "massless states in the open string field theory. Up to some contact terms, the\n",
      "results reproduce exactly the corresponding amplitudes in the bosonic string\n",
      "theory.\n",
      "\n",
      "1alishah@theory.ipm.ac.ir\n",
      "2garousi@theory.ipm.ac.ir\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Abstract\n",
      "It is important to reveal the brane-bulk correspondence for understanding the brane\n",
      "world cosmology. When gravitational waves exist in the bulk, however, it is difficult\n",
      "to make the analysis of the interrelationship between the brane and the bulk. Hence,\n",
      "the minimal model which allows gravitational waves in the bulk would be useful. As\n",
      "for such models, we adopt the Bianchi type midi-superspace models. In particular,\n",
      "the effects of gravitational waves in the bulk on the brane cosmology is examined\n",
      "using the midi-superspace approach.\n",
      "\n",
      "\n",
      "Abstract\n",
      "\n",
      "The analysis of (2+1)-dimensional Yang-Mills (YM2+1) theory via the use\n",
      "of gauge-invariant matrix variables is reviewed. The vacuum wavefunction,\n",
      "string tension, the propagator mass for gluons, its relation to the magnetic\n",
      "mass for YM3+1 at nonzero temperature and the extension of our analysis to\n",
      "the Yang-Mills-Chern-Simons theory are discussed. (Talk given at the Light-\n",
      "cone Workshop, Trento, 2001, to be published in Nucl. Phys. Proceedings\n",
      "and Supplements.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Abstract\n",
      "\n",
      "Explicit solutions for one completely-integrable system of Calogero-\n",
      "\n",
      "Moser type in external fields are found in case of three and four interacting\n",
      "\n",
      "particles. Relation between coupling constant, initial values of coordinates\n",
      "\n",
      "and time of falling into the singularity of potential is derived.\n",
      "\n",
      "\n",
      "Abstract\n",
      "\n",
      "In this work we discuss the effect of the quartic fermion self-interaction of\n",
      "\n",
      "Thirring type in QED in D = 2 and D = 3 dimensions. This is done through\n",
      "\n",
      "the computation of the effective action up to quadratic terms in the photon field.\n",
      "\n",
      "We analyze the corresponding nonlocal photon propagators nonperturbatively in\n",
      "k\n",
      "m\n",
      "\n",
      ", where k is the photon momentum and m the fermion mass. The poles of the\n",
      "\n",
      "propagators were determined numerically by using the Mathematica software. In\n",
      "\n",
      "D = 2 there is always a massless pole whereas for strong enough Thirring coupling\n",
      "\n",
      "a massive pole may appear . For D = 3 there are three regions in parameters space.\n",
      "\n",
      "We may have one or two massive poles or even no pole at all. The inter-quark\n",
      "\n",
      "static potential is computed analytically in D = 2. We notice that the Thirring\n",
      "\n",
      "interaction contributes with a screening term to the confining linear potential of\n",
      "\n",
      "massive QED2. In D = 3 the static potential must be calculated numerically. The\n",
      "\n",
      "screening nature of the massive QED3 prevails at any distance, indicating that this\n",
      "\n",
      "is a universal feature of D = 3 electromagnetic interaction. Our results become\n",
      "\n",
      "exact for an infinite number of fermion flavors.\n",
      "\n",
      "PACS-No.: 11.15.Bt , 11.15.-q\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdir = \"../Tex PDF/\"\n",
    "l = os.listdir(pdir)\n",
    "pdf = [pdir + e for e in l]\n",
    "\n",
    "for p in pdf:\n",
    "    print(get_abstract(p)) \n",
    "# raw = parser.from_file('Text Summarization with Pretrained Encoders.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
