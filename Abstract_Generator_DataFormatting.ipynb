{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8yRN30hAWKGQ",
    "outputId": "998581af-fd67-4341-fba8-85f1e98942ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm==4.36.1 in /usr/local/lib/python3.6/dist-packages (4.36.1)\n"
     ]
    }
   ],
   "source": [
    "# tqdm version 4.36.1 is required\n",
    "\n",
    "\n",
    "!pip install tqdm==4.36.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcsjCLNLt1-Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AxlIcG9tWV1Y",
    "outputId": "7b1d779e-7c6c-492d-eb00-d111e08e1d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mounting Drive\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "oeYeuULxWb4Z",
    "outputId": "4d3c1c18-6a6b-41de-a300-2d617cb0b6f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing Libaries\n",
    "\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "import glob\n",
    "from bs4 import BeautifulSoup \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "import keras\n",
    "import warnings\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TkenhppLWg-B"
   },
   "outputs": [],
   "source": [
    "# Progress bar\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M25zNardWsRI"
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "data = pd.read_csv('/content/drive/My Drive/dataset1.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "3a3pFVbRdc_j",
    "outputId": "aa8ad481-e124-4508-9a8c-44283daeceb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                        0\n",
       "MAIN-TITLE                        0\n",
       "HIGHLIGHTS                        0\n",
       "KEYPHRASES                        0\n",
       "ABSTRACT                          0\n",
       "ACKNOWLEDGEMENTS               6225\n",
       "REFERENCES                     8195\n",
       "INTRODUCTION                    254\n",
       "RESULTS AND DISCUSSION         7516\n",
       "CONCLUSION                     5441\n",
       "ACKNOWLEDGMENT                 7487\n",
       "CONCLUSIONS                    5340\n",
       "RELATED WORK                   7065\n",
       "DISCUSSION                     6202\n",
       "ACKNOWLEDGMENTS                5393\n",
       "OVERVIEW                       8059\n",
       "DISCUSSION AND CONCLUSIONS     8084\n",
       "EXPERIMENTAL RESULTS           7598\n",
       "METHODS                        7430\n",
       "RESULTS                        6165\n",
       "IMPLEMENTATION                 8047\n",
       "ACKNOWLEDGEMENT                7715\n",
       "MATERIAL AND METHODS           8023\n",
       "CONCLUSIONS AND FUTURE WORK    7894\n",
       "BACKGROUND                     7782\n",
       "METHODOLOGY                    7850\n",
       "FUTURE WORK                    8090\n",
       "EXPERIMENTS                    7764\n",
       "LITERATURE REVIEW              7686\n",
       "EVALUATION                     7979\n",
       "RELATED WORKS                  7984\n",
       "ANALYSIS                       8091\n",
       "METHOD                         7840\n",
       "PROCEDURE                      7991\n",
       "MOTIVATION                     8083\n",
       "LIMITATIONS                    7976\n",
       "DISCUSSION AND CONCLUSION      8063\n",
       "SUMMARY                        7954\n",
       "PROPOSED METHOD                8114\n",
       "DISCUSSIONS                    8103\n",
       "PERFORMANCE EVALUATION         8072\n",
       "RESULTS AND DISCUSSIONS        8096\n",
       "EXPERIMENTAL DESIGN            8070\n",
       "EXPERIMENTS AND RESULTS        8062\n",
       "RESULTS AND ANALYSIS           8139\n",
       "IMPLEMENTATION DETAILS         8128\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(subset = ['ABSTRACT'], inplace = True)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4vpD4SF4gjaq",
    "outputId": "f4a661dd-d026-4d40-eaa9-14d4c01f7161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE : (8196, 46)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>MAIN-TITLE</th>\n",
       "      <th>HIGHLIGHTS</th>\n",
       "      <th>KEYPHRASES</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>ACKNOWLEDGEMENTS</th>\n",
       "      <th>REFERENCES</th>\n",
       "      <th>INTRODUCTION</th>\n",
       "      <th>RESULTS AND DISCUSSION</th>\n",
       "      <th>CONCLUSION</th>\n",
       "      <th>ACKNOWLEDGMENT</th>\n",
       "      <th>CONCLUSIONS</th>\n",
       "      <th>RELATED WORK</th>\n",
       "      <th>DISCUSSION</th>\n",
       "      <th>ACKNOWLEDGMENTS</th>\n",
       "      <th>OVERVIEW</th>\n",
       "      <th>DISCUSSION AND CONCLUSIONS</th>\n",
       "      <th>EXPERIMENTAL RESULTS</th>\n",
       "      <th>METHODS</th>\n",
       "      <th>RESULTS</th>\n",
       "      <th>IMPLEMENTATION</th>\n",
       "      <th>ACKNOWLEDGEMENT</th>\n",
       "      <th>MATERIAL AND METHODS</th>\n",
       "      <th>CONCLUSIONS AND FUTURE WORK</th>\n",
       "      <th>BACKGROUND</th>\n",
       "      <th>METHODOLOGY</th>\n",
       "      <th>FUTURE WORK</th>\n",
       "      <th>EXPERIMENTS</th>\n",
       "      <th>LITERATURE REVIEW</th>\n",
       "      <th>EVALUATION</th>\n",
       "      <th>RELATED WORKS</th>\n",
       "      <th>ANALYSIS</th>\n",
       "      <th>METHOD</th>\n",
       "      <th>PROCEDURE</th>\n",
       "      <th>MOTIVATION</th>\n",
       "      <th>LIMITATIONS</th>\n",
       "      <th>DISCUSSION AND CONCLUSION</th>\n",
       "      <th>SUMMARY</th>\n",
       "      <th>PROPOSED METHOD</th>\n",
       "      <th>DISCUSSIONS</th>\n",
       "      <th>PERFORMANCE EVALUATION</th>\n",
       "      <th>RESULTS AND DISCUSSIONS</th>\n",
       "      <th>EXPERIMENTAL DESIGN</th>\n",
       "      <th>EXPERIMENTS AND RESULTS</th>\n",
       "      <th>RESULTS AND ANALYSIS</th>\n",
       "      <th>IMPLEMENTATION DETAILS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n\\n               \\n                  \\n                  \\n                     \\n                        \\n                           \\n                           We develop an evolutionary app...</td>\n",
       "      <td>\\nAgent based problems\\n\\nComplementarity conditions\\n\\nEvolutionary algorithm\\n\\nParallel search\\n\\n</td>\n",
       "      <td>\\n\\n               \\n               \\n                  Deterministic approaches to simultaneously solve different interrelated optimisation problems lead to a general class of nonlinear complemen...</td>\n",
       "      <td>\\nSix anonymous reviewers provided comments and suggestions that strongly improved the content and presentation of the paper. All errors or omissions are the authors alone.\\n\\n</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n\\n               \\n               \\n                  \\n                     \\n                        \\n                           \\n                           The aggregated artificial neural ...</td>\n",
       "      <td>\\nScaffolds\\n\\n3D printer\\n\\nAggregated artificial neural network (AANN)\\n\\nParticle swarm optimization (PSO)\\n\\nPorous structure\\n\\nMechanical strength\\n\\n</td>\n",
       "      <td>\\n\\n               \\n               \\n                  Fabrication of three-dimensional structures has gained increasing importance in the bone tissue engineering (BTE) field. Mechanical properti...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\nAdditive manufacturing (AM) is a layer-over-layer manufacturing technique. In most cases, enables complex components to be manufactured that are difficult to fabricate or cannot be made using co...</td>\n",
       "      <td>\\nIn this section, a predictive model for 3DP process is extracted. The model predicts the mechanical strength, and the open porosity of a part fabricated using this process. Mechanical strength a...</td>\n",
       "      <td>\\nIn this study, the AANN was used to investigate the simultaneous effects of layer thickness, delay time between spreading each layer, and print orientation on the compressive strength and porosi...</td>\n",
       "      <td>\\nThis study was supported by the High Impact Research Grant from the Ministry of Higher Education of Malaysia (UM.C/HIR/MOHE/ENG/10 D000010-16001).\\n\\n</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>220</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n\\n               \\n                  \\n                  \\n                     \\n                        \\n                           \\n                           A stochastic global optimizati...</td>\n",
       "      <td>\\nOpen pit mine design\\n\\nGlobal optimization\\n\\nProduction scheduling\\n\\nMetaheuristics\\n\\nDestination policy\\n\\n</td>\n",
       "      <td>\\n\\n               \\n               \\n                  Global optimization for mining complexes aims to generate a production schedule for the various mines and processing streams that maximizes ...</td>\n",
       "      <td>\\nThe work in this paper was funded by NSERC CRD 411270, NSERC Discovery Grant 239019, and the industry members of the COSMO Stochastic Mine Planning Laboratory: AngloGold Ashanti, Barrick Gold, B...</td>\n",
       "      <td></td>\n",
       "      <td>\\nGlobal optimization for mining complexes addresses the issue of integrated mining and processing operations with multiple pits or underground mines, multiple metals or minerals, stockpiles, blen...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\nThis work presents a framework for global asset optimization of mining complexes under uncertainty, whereby the solutions provide robust long-term open-pit mine extraction sequences and destinat...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>287</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n\\n               \\n                  \\n                  \\n                     \\n                        \\n                           \\n                           High-dimensional biological da...</td>\n",
       "      <td>\\nFuzzy systems\\n\\nSupport vector regression\\n\\nPeptide binding affinity\\n\\n</td>\n",
       "      <td>\\n\\n               \\n               \\n                  Support vector machines have a wide use for the prediction problems in life sciences. It has been shown to offer more generalisation ability...</td>\n",
       "      <td>\\nDuring this study, Volkan Uslan was funded by De Montfort University Leicester with full PhD tuition fee scholarship. The authors thank to Dr Ovidiu Ivanciuc for organising the CoEPrA contest th...</td>\n",
       "      <td></td>\n",
       "      <td>\\nPeptide binding plays vital roles in the molecular biology of the cell. The process of the peptide binding can activate the cytotoxic T-cells in the immune system [1]. One of the most challengin...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\nIn this paper, a hybrid system (TSK-SVR) that has helped improve the predictive ability of TSK-FS significantly with the aid of support-based vector method was developed and demonstrated with th...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n\\n               \\n                  \\n                  \\n                     \\n                        \\n                           \\n                           Few graph layout methods captu...</td>\n",
       "      <td>\\nSmall world networks\\n\\nAdjacency matrix\\n\\nNode attributes\\n\\nGraph visualization\\n\\nTargeted projection pursuit\\n\\n</td>\n",
       "      <td>\\n\\n               \\n               \\n                  Many networks exhibit small-world properties. The structure of a small-world network is characterized by short average path lengths and high...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\nSmall-world networks are a commonly occurring graph structure characterized by short average path lengths and high clustering coefficients [1]. This means that even when the network is large the...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\nThis paper has presented an extension to the small-worlds pilot study presented in Gibson and Faith [8] where graphTPP was used to lay out a small-world network using node attributes. In this ca...</td>\n",
       "      <td>\\nSmall-world networks are characterized by short average path lengths (the shortest path between any pair of nodes) and high clustering coefficients (e.g., in social networks this would be the nu...</td>\n",
       "      <td>\\nThe structure of a small-world network should make it perfectly suited to visualization. Representing the communities that form within the graph should be a principal aim for layout if we want t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 MAIN-TITLE  ... RESULTS AND ANALYSIS IMPLEMENTATION DETAILS\n",
       "33           33         \\n  ...                                            \n",
       "197         197         \\n  ...                                            \n",
       "220         220         \\n  ...                                            \n",
       "287         287         \\n  ...                                            \n",
       "298         298         \\n  ...                                            \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(value = \"\", inplace= True)\n",
    "print(\"SHAPE : \"+ str(data.shape))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "KHIrxK3phVNE",
    "outputId": "09aa4931-87e0-4590-f9f5-4d36d890899f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT',\n",
       "       'ACKNOWLEDGEMENTS', 'REFERENCES', 'INTRODUCTION',\n",
       "       'RESULTS AND DISCUSSION', 'CONCLUSION', 'ACKNOWLEDGMENT', 'CONCLUSIONS',\n",
       "       'RELATED WORK', 'DISCUSSION', 'ACKNOWLEDGMENTS', 'OVERVIEW',\n",
       "       'DISCUSSION AND CONCLUSIONS', 'EXPERIMENTAL RESULTS', 'METHODS',\n",
       "       'RESULTS', 'IMPLEMENTATION', 'ACKNOWLEDGEMENT', 'MATERIAL AND METHODS',\n",
       "       'CONCLUSIONS AND FUTURE WORK', 'BACKGROUND', 'METHODOLOGY',\n",
       "       'FUTURE WORK', 'EXPERIMENTS', 'LITERATURE REVIEW', 'EVALUATION',\n",
       "       'RELATED WORKS', 'ANALYSIS', 'METHOD', 'PROCEDURE', 'MOTIVATION',\n",
       "       'LIMITATIONS', 'DISCUSSION AND CONCLUSION', 'SUMMARY',\n",
       "       'PROPOSED METHOD', 'DISCUSSIONS', 'PERFORMANCE EVALUATION',\n",
       "       'RESULTS AND DISCUSSIONS', 'EXPERIMENTAL DESIGN',\n",
       "       'EXPERIMENTS AND RESULTS', 'RESULTS AND ANALYSIS',\n",
       "       'IMPLEMENTATION DETAILS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsiuC_VKr7vk"
   },
   "outputs": [],
   "source": [
    "data.to_csv('/content/drive/My Drive/dataset1.csv',index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "b6bGFlDjov8p",
    "outputId": "1e1a4e60-a753-4c68-f95e-870d309366a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT',\n",
       "       'ACKNOWLEDGEMENTS', 'REFERENCES', 'INTRODUCTION', 'RELATED WORK',\n",
       "       'OVERVIEW', 'METHODS', 'IMPLEMENTATION', 'MATERIAL AND METHODS',\n",
       "       'BACKGROUND', 'METHODOLOGY', 'EXPERIMENTS', 'LITERATURE REVIEW',\n",
       "       'EVALUATION', 'RELATED WORKS', 'ANALYSIS', 'METHOD', 'PROCEDURE',\n",
       "       'MOTIVATION', 'LIMITATIONS', 'DISCUSSION AND CONCLUSION',\n",
       "       'PROPOSED METHOD', 'PERFORMANCE EVALUATION', 'EXPERIMENTAL DESIGN',\n",
       "       'EXPERIMENTS AND RESULTS', 'IMPLEMENTATION DETAILS',\n",
       "       'RESULT | CONCLUSION | DISCUSSION'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['combined'] = df[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "data['ACKNOWLEDGEMENTS'] = data['ACKNOWLEDGEMENTS'] + data['ACKNOWLEDGEMENT']\n",
    "data['ACKNOWLEDGEMENTS'] = data['ACKNOWLEDGEMENTS']+data['ACKNOWLEDGEMENT']\n",
    "data['ACKNOWLEDGEMENTS'] = data['ACKNOWLEDGEMENTS']+data['ACKNOWLEDGMENTS']\n",
    "data.drop(['ACKNOWLEDGMENT','ACKNOWLEDGEMENT','ACKNOWLEDGMENTS'],axis = 1,inplace= True)\n",
    "\n",
    "data['RESULT | CONCLUSION | DISCUSSION'] = data['RESULTS AND DISCUSSION']+data['CONCLUSION']+data['CONCLUSIONS']+data['DISCUSSION']+data['DISCUSSION AND CONCLUSIONS']+data['EXPERIMENTAL RESULTS'] + data['RESULTS']+data['CONCLUSIONS AND FUTURE WORK']+data['FUTURE WORK']+data['SUMMARY']+data['DISCUSSIONS']+data['RESULTS AND DISCUSSIONS']+data['RESULTS AND ANALYSIS']\n",
    "data.drop(['RESULTS AND DISCUSSION','CONCLUSION','CONCLUSIONS','DISCUSSION','DISCUSSION AND CONCLUSIONS','EXPERIMENTAL RESULTS','RESULTS','CONCLUSIONS AND FUTURE WORK','FUTURE WORK','SUMMARY','DISCUSSIONS','RESULTS AND DISCUSSIONS','RESULTS AND ANALYSIS'],axis =1 ,inplace = True)\n",
    "\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yZnezH2nywpB",
    "outputId": "b2a283cd-6f3e-4245-8f51-2b7f98464be7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8196, 28)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['METHOD'] = data['METHODS']+data['MATERIAL AND METHODS']+data['METHODOLOGY']+data['METHOD']\n",
    "data.drop(['MATERIAL AND METHODS','METHODOLOGY','METHODS'],axis = 1, inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "FHogqT7joOA1",
    "outputId": "71352395-2b83-4398-919e-9d7aeb7a87bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT',\n",
       "       'ACKNOWLEDGEMENTS', 'REFERENCES', 'INTRODUCTION', 'RELATED WORK',\n",
       "       'OVERVIEW', 'IMPLEMENTATION', 'BACKGROUND', 'EXPERIMENTS',\n",
       "       'LITERATURE REVIEW', 'EVALUATION', 'RELATED WORKS', 'ANALYSIS',\n",
       "       'METHOD', 'PROCEDURE', 'MOTIVATION', 'LIMITATIONS',\n",
       "       'DISCUSSION AND CONCLUSION', 'PROPOSED METHOD',\n",
       "       'PERFORMANCE EVALUATION', 'EXPERIMENTAL DESIGN',\n",
       "       'EXPERIMENTS AND RESULTS', 'IMPLEMENTATION DETAILS',\n",
       "       'RESULT | CONCLUSION | DISCUSSION'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhsUoQ7PwYso"
   },
   "outputs": [],
   "source": [
    "data['METHOD'] = data['METHOD'] + data['PROPOSED METHOD']\n",
    "data['RESULT | CONCLUSION | DISCUSSION'] = data['RESULT | CONCLUSION | DISCUSSION'] + data['PERFORMANCE EVALUATION'] + data['ANALYSIS'] + data['EXPERIMENTS AND RESULTS']\n",
    "data['IMPLEMENTATION'] = data['IMPLEMENTATION'] + data['IMPLEMENTATION DETAILS'] + data['PROCEDURE'] \n",
    "data['RELATED WORK'] = data['RELATED WORK'] + data['RELATED WORKS'] + data['LITERATURE REVIEW'] + data['BACKGROUND']  \n",
    "\n",
    "data.drop(['PROPOSED METHOD','PERFORMANCE EVALUATION','ANALYSIS','EXPERIMENTS AND RESULTS','IMPLEMENTATION DETAILS','PROCEDURE','RELATED WORKS','LITERATURE REVIEW','BACKGROUND'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqrcK6w-3R6x"
   },
   "outputs": [],
   "source": [
    "data.to_csv('/content/drive/My Drive/dataset1.csv',index = False, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "rIeip7jl4fFZ",
    "outputId": "82e04cdf-8971-4d49-f351-bf208dbec04b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT',\n",
       "       'ACKNOWLEDGEMENTS', 'REFERENCES', 'INTRODUCTION', 'RELATED WORK',\n",
       "       'OVERVIEW', 'IMPLEMENTATION', 'EXPERIMENTS', 'EVALUATION', 'METHOD',\n",
       "       'MOTIVATION', 'LIMITATIONS', 'DISCUSSION AND CONCLUSION',\n",
       "       'EXPERIMENTAL DESIGN', 'RESULT | CONCLUSION | DISCUSSION'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCYgrs2i4Wla"
   },
   "outputs": [],
   "source": [
    "data['RESULT | CONCLUSION | DISCUSSION'] = data['RESULT | CONCLUSION | DISCUSSION'] + data['DISCUSSION AND CONCLUSION'] + data['EVALUATION']\n",
    "data['EXPERIMENTS'] = data['EXPERIMENTS'] + data['EXPERIMENTAL DESIGN']\n",
    "\n",
    "data.drop(['DISCUSSION AND CONCLUSION','EVALUATION','EXPERIMENTS','EXPERIMENTAL DESIGN'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "L9wWHb22501m",
    "outputId": "3b97fcd1-01e6-40c8-966e-c76c711d32f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8196, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'MAIN-TITLE', 'HIGHLIGHTS', 'KEYPHRASES', 'ABSTRACT',\n",
       "       'ACKNOWLEDGEMENTS', 'REFERENCES', 'INTRODUCTION', 'RELATED WORK',\n",
       "       'OVERVIEW', 'IMPLEMENTATION', 'METHOD', 'MOTIVATION', 'LIMITATIONS',\n",
       "       'RESULT | CONCLUSION | DISCUSSION'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1u2ZPz758yW"
   },
   "outputs": [],
   "source": [
    "data['BODY'] = data['INTRODUCTION']+data['RELATED WORK']+data['OVERVIEW']+data['MOTIVATION'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sO0L7PF26R-x"
   },
   "outputs": [],
   "source": [
    "data.to_csv('/content/drive/My Drive/dataset1.csv',index = False, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_T-WPfu-7B5L"
   },
   "outputs": [],
   "source": [
    "data['Body_LENGTH'] = data.apply(lambda row: len(row['BODY']),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJdxYmNT7nUn"
   },
   "outputs": [],
   "source": [
    "data.to_csv('/content/drive/My Drive/dataset1.csv',index = False, header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJ8YNc9JWudo"
   },
   "outputs": [],
   "source": [
    "# Preprocessing \"body\" text\n",
    "\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    " \n",
    "def clean_body(text):\n",
    "    newText = text.lower()\n",
    "    newText = re.sub('[^\\w\\s\\d\\.]','',newText)\n",
    "    newText = ' '.join(newText.split())\n",
    "    tokens = [w for w in newText.split() if not w in STOP_WORDS]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_body = []\n",
    "for t in data['body']:\n",
    "    cleaned_body.append(clean_body(t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXL_mAaPW9fY"
   },
   "outputs": [],
   "source": [
    "# Preprocessing \"highlight\" text\n",
    "\n",
    "\n",
    "def clean_highlight(text):\n",
    "  newText = text.lower()\n",
    "  newText = re.sub('[^\\w\\s\\d\\.]','',newText)\n",
    "  newText = ' '.join(newText.split())\n",
    "  newText = '_START_ '+ newText + ' _END_'\n",
    "  return newText\n",
    "\n",
    "cleaned_highlight = []\n",
    "for t in data['highlights']:\n",
    "    cleaned_highlight.append(clean_highlight(t))\n",
    "\n",
    "cleaned_highlight[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4v2miObtW_lE"
   },
   "outputs": [],
   "source": [
    "# Storing preprocessed data in the dataframe\n",
    "\n",
    "\n",
    "data['cleaned_highlights'] = cleaned_highlight\n",
    "data['cleaned_body'] = cleaned_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfQR9pLNXKZz"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zph3L3GQXPFq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "for i in data['cleaned_body']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_highlights']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'Body':text_word_count, 'Highlights':summary_word_count})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7xA5CIoXS0G"
   },
   "outputs": [],
   "source": [
    "max_len_body = 1000\n",
    "max_len_highlight = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5N9mwVwXZWI"
   },
   "outputs": [],
   "source": [
    "# Converting cleaned data into strings\n",
    "\n",
    "\n",
    "data.cleaned_body = data.cleaned_body.progress_apply(lambda x: str(x))\n",
    "data.cleaned_highlights = data.cleaned_highlights.progress_apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKo2IrqhXfJs"
   },
   "outputs": [],
   "source": [
    "# Splitting data into training and test sets\n",
    "# Test set is 20% of total data\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(data['cleaned_body'],data['cleaned_highlights'],test_size=0.2,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K10GoGXpXia4"
   },
   "outputs": [],
   "source": [
    "# Tokenizing \"body\"\n",
    "x_tok = Tokenizer()\n",
    "x_tok.fit_on_texts(list(x_train))\n",
    "\n",
    "# Converting text to number sequences\n",
    "x_train = x_tok.texts_to_sequences(x_train) \n",
    "x_test = x_tok.texts_to_sequences(x_test)\n",
    "\n",
    "# Padding zero upto maximum length\n",
    "x_train = pad_sequences(x_train,  maxlen=max_len_body, padding='post') \n",
    "x_test = pad_sequences(x_test, maxlen=max_len_body, padding='post')\n",
    "\n",
    "# Total number of words\n",
    "x_vocab_size = len(x_tok.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YS-cttoRXnIM"
   },
   "outputs": [],
   "source": [
    "# Tokenizing \"highlights\"\n",
    "y_tok = Tokenizer()\n",
    "y_tok.fit_on_texts(list(y_train))\n",
    "\n",
    "# Converting text to number sequences\n",
    "y_train = y_tok.texts_to_sequences(y_train) \n",
    "y_test = y_tok.texts_to_sequences(y_test)\n",
    "\n",
    "# Padding zero upto maximum length\n",
    "y_train = pad_sequences(y_train,  maxlen=max_len_highlight, padding='post') \n",
    "y_test = pad_sequences(y_test, maxlen=max_len_highlight, padding='post')\n",
    "\n",
    "# Word count\n",
    "y_vocab_size = len(y_tok.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQ_h5wQ4XwPd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
    "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
    "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKJJowBPX04V"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "\n",
    "\n",
    "K.clear_session() \n",
    "latent_dim = 50 \n",
    "\n",
    "\n",
    "# Encoder \n",
    "encoder_inputs = Input(shape=(max_len_body,)) \n",
    "enc_emb = Embedding(x_vocab_size, latent_dim,trainable=True)(encoder_inputs) \n",
    "\n",
    "# 1st LSTM Layer\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "# 2nd LSTM Layer\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "# 3rd LSTM Layer\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
    "\n",
    "# Decoder \n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(y_vocab_size, latent_dim,trainable=True) \n",
    "dec_emb = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "# LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "# Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_vocab_size, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input) \n",
    "\n",
    "# Model Definition\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvBdWresX4yR"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADAInMJTX7A_"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-6VW0p2X9iN"
   },
   "outputs": [],
   "source": [
    "history=model.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:], epochs=30, callbacks=[es], batch_size=128, validation_data=([x_test,y_test[:,:-1]], y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ThmAJmJYCVd"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/My Drive/NLP Project/Project Final/model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbOxpRf8YEwM"
   },
   "outputs": [],
   "source": [
    "# Visualizing training ans test loss functions\n",
    "\n",
    "\n",
    "from matplotlib import pyplot \n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9yqFK5YaYHK3"
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tok.index_word \n",
    "reverse_source_word_index=x_tok.index_word \n",
    "target_word_index=y_tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2XZJiplYJtx"
   },
   "outputs": [],
   "source": [
    "# Encoder Inference\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder Inference\n",
    "# Below tensors hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_len_body,latent_dim))\n",
    "\n",
    "# Getting decoder sequence embeddings\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Predicting the next word in the sequence\n",
    "# Setting the initial states to the previous time step states\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# Attention Inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# Dense softmax layer to calculate probability distribution over target vocab\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final Decoder model\n",
    "decoder_model = Model(\n",
    "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXxr6YgIYOY-"
   },
   "outputs": [],
   "source": [
    "# Function to implement inference\n",
    "\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encoding input as state vectors\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generating empty target sequence of length 1\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Taking the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        try:\n",
    "            sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        except:\n",
    "            sampled_token = reverse_target_word_index[np.random.randint(1, len(reverse_target_word_index))]\n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "            # Exit condition: either hit max length or find stop word.\n",
    "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_highlight-1)):\n",
    "                stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IqsSDZ3mYTGM"
   },
   "outputs": [],
   "source": [
    "def seq2highlights(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
    "        newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-9tALmiYWKx"
   },
   "outputs": [],
   "source": [
    "reference = []\n",
    "hypothesis = []\n",
    "for i in range(10):\n",
    "  print(\"Highlights:\")\n",
    "  print(seq2summary(y_test[i]))\n",
    "  reference.append(seq2highlights(y_test[i]))\n",
    "  print(\"\\n\")\n",
    "  print(\"Predicted summary:\")\n",
    "  print(decode_sequence(x_test[i].reshape(1,max_len_body)))\n",
    "  hypothesis.append(decode_sequence(x_test[i].reshape(1,max_len_body)))\n",
    "  print(\"\\n\")\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WtZGcwjrYZ-a"
   },
   "outputs": [],
   "source": [
    "! pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGmZxS3zYd_P"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "score = Rouge()\n",
    "score.get_scores(hypothesis, reference, avg = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K4LT351hYmLk"
   },
   "outputs": [],
   "source": [
    "# Reading papers, cleaning, creating & pickling dataframe\n",
    "\n",
    "\n",
    "def read_paper(path):\n",
    "  \"\"\"\n",
    "  Reads research papers and store them in a string\n",
    "  \"\"\"\n",
    "  f = open(path, 'r', encoding=\"utf-8\")\n",
    "  text = str(f.read())\n",
    "  f.close()\n",
    "  return text\n",
    "\n",
    "\n",
    "def create_dataframe(NO_INPUT_PAPERS):\n",
    "  \"\"\"\n",
    "  Takes number of papers to read and stores data in a dataframe\n",
    "  \"\"\"\n",
    "  temp_papers = []\n",
    "  filenames = []\n",
    "  for filename in tqdm(glob.glob(\"/content/drive/My Drive/NLP Project/Project Final/Parsed_Papers/*.txt\")[:NO_INPUT_PAPERS]):\n",
    "      temp_papers.append(read_paper(filename))\n",
    "      filenames.append(filename)\n",
    "  return pd.DataFrame(list(zip(temp_papers, filenames)), columns =['text', 'filenames'])\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "  \"\"\"\n",
    "  Removes unwanted characters, accounting for unicode characters\n",
    "  \"\"\"\n",
    "  text = re.sub(\"@&#\", \" \", text)\n",
    "  text = re.sub(\"\\n\", \" \", text)\n",
    "  text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "  return text\n",
    "\n",
    "\n",
    "data = create_dataframe(NO_INPUT_PAPERS = 10000)\n",
    "\n",
    "data['highlights'] = data['text'].progress_apply(lambda x: re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', x,  flags = re.I)[0])\n",
    "data = data[data.highlights != '    ']\n",
    "\n",
    "data['body'] = data['text'].progress_apply(lambda x: re.findall(r'.*(?:abstract )(.*?)$', x,  flags = re.I)[0])\n",
    "\n",
    "data.to_pickle(\"./papers.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SlJbYJiSY1xp"
   },
   "outputs": [],
   "source": [
    "# Function to find filename which don't have \"highlights\" in it\n",
    "\n",
    "\n",
    "def file_name(NO_INPUT_PAPERS):\n",
    "  temp_papers = []\n",
    "  for filename in glob.glob(\"/content/drive/My Drive/NLP Project/Project Final/Parsed_Papers/*.txt\")[:NO_INPUT_PAPERS]:\n",
    "    text = read_paper(filename)\n",
    "    text = re.sub(\"@&#\", \" \", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "    highlights = re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', text,  flags = re.I)\n",
    "    if highlights == []:\n",
    "      print(filename)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Abstract Generator DataFormatting",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
